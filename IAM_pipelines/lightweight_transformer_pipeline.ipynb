{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Transformer-based Model for Handwritten Character Recognition \n",
    "(https://hal.science/hal-03685976/file/A_Light_Transformer_Based_Architecture_for_Handwritten_Text_Recognition.pdf)\n",
    "\n",
    "## ***note: Has a CNN backbone***\n",
    "\n",
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture  \n",
    "Build up with a double Transformer architecture:  \n",
    "- Image transformer as encoder: Extracts the visual features\n",
    "- Text transformer as decoder: Language modeling\n",
    "- Encoder: \n",
    "- Decoder: Generates word-sections sequence using visual features and previous predictions\n",
    "\n",
    "### Encoder:  \n",
    "- CNN Backbone (5 convolutions)\n",
    "- Sinusodial position encoding  \n",
    "- 4 layer transformer layer encoder\n",
    "\n",
    "### Decoder: \n",
    "- Takes encoder output and along with sequence of previously predicted characters\n",
    "- Additional loss in the middle of the network to help convergence\n",
    "\n",
    "--------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_image\n",
    "import torchvision as tv\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"/home/hkolstee/uniprojects/DATA/HWR/IAM-data/IAM-data/\"\n",
    "TRAIN_TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "INPUT_HEIGHT = 128\n",
    "# input width -> largest width in batch\n",
    "# padded to get to width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_names</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a03-017-07.png</td>\n",
       "      <td>into the pro-communist north and the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a03-017-05.png</td>\n",
       "      <td>to 1958 kept the kingdom in peace, though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a03-017-08.png</td>\n",
       "      <td>pro-western centre and south.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a03-017-02.png</td>\n",
       "      <td>in Phnom Penh indicate that he still regards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a03-017-06.png</td>\n",
       "      <td>at the cost of virtual partition of the country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7453</th>\n",
       "      <td>d06-000-08.png</td>\n",
       "      <td>fears are based upon completely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7454</th>\n",
       "      <td>d06-000-05.png</td>\n",
       "      <td>is worrying them, to find the original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7455</th>\n",
       "      <td>d06-000-09.png</td>\n",
       "      <td>irrational pre-conceived notions - or to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7456</th>\n",
       "      <td>d06-000-02.png</td>\n",
       "      <td>already suggested, not to be silly or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7457</th>\n",
       "      <td>d06-000-00.png</td>\n",
       "      <td>In the first place it is not a great deal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7458 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           img_names                                           labels\n",
       "0     a03-017-07.png             into the pro-communist north and the\n",
       "1     a03-017-05.png        to 1958 kept the kingdom in peace, though\n",
       "2     a03-017-08.png                    pro-western centre and south.\n",
       "3     a03-017-02.png     in Phnom Penh indicate that he still regards\n",
       "4     a03-017-06.png  at the cost of virtual partition of the country\n",
       "...              ...                                              ...\n",
       "7453  d06-000-08.png                  fears are based upon completely\n",
       "7454  d06-000-05.png           is worrying them, to find the original\n",
       "7455  d06-000-09.png         irrational pre-conceived notions - or to\n",
       "7456  d06-000-02.png            already suggested, not to be silly or\n",
       "7457  d06-000-00.png        In the first place it is not a great deal\n",
       "\n",
       "[7458 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_fwf(DATA_PATH + \"iam_lines_gt.txt\", header = None)\n",
    "raw_data = raw_data.values.tolist()\n",
    "\n",
    "data = {'img_names': np.squeeze(raw_data[::2]),\n",
    "        'labels': np.squeeze(raw_data[1::2])}\n",
    "        # 'labels': [\"<BOS>\" + str(list(string)) + \"<EOS>\" for string in np.squeeze(raw_data[1::2])]}\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['labels'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Data augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = TRAIN_TEST_SPLIT)\n",
    "\n",
    "# reset indices from current random state\n",
    "train.reset_index(inplace = True)\n",
    "test.reset_index(inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom pytorch dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we need the input/image width we have to resize the images to.  \n",
    "This is the largest image width in the entire batch of images (source paper randomly added/removed new augments each training epoch).   \n",
    "For now we just take the largest width in the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBiggestWidth(data: pd.DataFrame):\n",
    "    biggest_width = 0\n",
    "\n",
    "    for index in range(len(data['img_names'])):\n",
    "        image_path = os.path.join(DATA_PATH, \"img\", data['img_names'][index])\n",
    "        image = read_image(image_path)\n",
    "        \n",
    "        if (image.size(2) > biggest_width):\n",
    "            biggest_width = image.size(2)\n",
    "\n",
    "    return biggest_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260\n"
     ]
    }
   ],
   "source": [
    "input_width = getBiggestWidth(data)\n",
    "print(input_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandWritingDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, img_width, img_height, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # function to get patches of tokens from original input image\n",
    "    # def __getTokens__(self, image):\n",
    "        # pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # input image\n",
    "        image_path = os.path.join(DATA_PATH, \"img\", self.data['img_names'][index])\n",
    "        # torchvision read_image call\n",
    "        image = read_image(image_path)\n",
    "        # resize to \n",
    "        \n",
    "        # string label\n",
    "        label = self.data['labels'][index]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        # return length of column\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HandWritingDataset(train, input_width, INPUT_HEIGHT, BATCH_SIZE)\n",
    "test_set = HandWritingDataset(test, input_width, INPUT_HEIGHT, BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinusodial positional encoding  \n",
    "(can be changed to nn.embedding layers if we don't get good results, however that is not exactly sinusodial pos encoding like in the paper I think)\n",
    "\n",
    "<!-- **CHANGED TO NN.EMBEDDING IN MODEL**   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinPosEncoding(nn.Module):\n",
    "    def __init__(self, dimensionality):\n",
    "        super(SinPosEncoding, self).__init__()\n",
    "        self.dims = dimensionality\n",
    "        self.max_len = 5000\n",
    "\n",
    "        # position vector\n",
    "        positions = torch.arange(0, self.max_len).unsqueeze(1)\n",
    "        # calculate added angle for sin/cos\n",
    "        angle = torch.exp(torch.arange(0, self.dims, 1) * (-np.log(10000.0) / self.dims))\n",
    "\n",
    "        # initialize the 2D positional encodings array\n",
    "        pos_encodings = torch.zeros(self.max_len, 1, self.dims)\n",
    "        # calucalte encodings\n",
    "        pos_encodings = torch.sin(positions * angle)\n",
    "\n",
    "        # add to buffer for training performance (?)\n",
    "        self.register_buffer('pos_encodings', pos_encodings)\n",
    "\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # adds the positional encoding elementwise to the tensor (seqlength, batch, embeddims)\n",
    "        input += self.pos_encodings[0:input.size(1)]\n",
    "\n",
    "        return input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For character level embedding (decoder input) we find out how many characters are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dict of uniques chars sorted on how common they are in the dataset labels\n",
    "def uniqueCharsByMostCommon(data: pd.DataFrame):\n",
    "    sortedDict = OrderedDict(Counter(''.join(data['labels'].values[4:-4])).most_common())\n",
    "    newDict = {}\n",
    "    \n",
    "    # first add begin of sentence and and of sentence tokens\n",
    "    newDict[\"<BOS>\"] = 0\n",
    "    newDict[\"<EOS>\"] = 1\n",
    "    \n",
    "    for idx, char in enumerate(sortedDict):\n",
    "        newDict[char] = idx + 2\n",
    "    \n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<BOS>': 0,\n",
       " '<EOS>': 1,\n",
       " ' ': 2,\n",
       " 'e': 3,\n",
       " 't': 4,\n",
       " 'a': 5,\n",
       " 'o': 6,\n",
       " 'n': 7,\n",
       " 'i': 8,\n",
       " 's': 9,\n",
       " 'r': 10,\n",
       " 'h': 11,\n",
       " 'l': 12,\n",
       " 'd': 13,\n",
       " 'c': 14,\n",
       " 'u': 15,\n",
       " 'm': 16,\n",
       " 'f': 17,\n",
       " 'w': 18,\n",
       " 'p': 19,\n",
       " 'g': 20,\n",
       " 'y': 21,\n",
       " 'b': 22,\n",
       " '.': 23,\n",
       " ',': 24,\n",
       " 'v': 25,\n",
       " 'k': 26,\n",
       " \"'\": 27,\n",
       " '\"': 28,\n",
       " '-': 29,\n",
       " 'T': 30,\n",
       " 'I': 31,\n",
       " 'M': 32,\n",
       " 'A': 33,\n",
       " 'S': 34,\n",
       " 'B': 35,\n",
       " 'P': 36,\n",
       " 'H': 37,\n",
       " 'W': 38,\n",
       " 'C': 39,\n",
       " 'N': 40,\n",
       " 'G': 41,\n",
       " 'x': 42,\n",
       " 'R': 43,\n",
       " 'L': 44,\n",
       " 'E': 45,\n",
       " 'D': 46,\n",
       " 'F': 47,\n",
       " '0': 48,\n",
       " '1': 49,\n",
       " 'j': 50,\n",
       " 'O': 51,\n",
       " 'q': 52,\n",
       " '!': 53,\n",
       " 'U': 54,\n",
       " '(': 55,\n",
       " 'K': 56,\n",
       " '?': 57,\n",
       " 'z': 58,\n",
       " '3': 59,\n",
       " ')': 60,\n",
       " ';': 61,\n",
       " '9': 62,\n",
       " 'V': 63,\n",
       " '2': 64,\n",
       " 'J': 65,\n",
       " 'Y': 66,\n",
       " ':': 67,\n",
       " '5': 68,\n",
       " '8': 69,\n",
       " '4': 70,\n",
       " '6': 71,\n",
       " '#': 72,\n",
       " '&': 73,\n",
       " '7': 74,\n",
       " '/': 75,\n",
       " 'Q': 76,\n",
       " 'X': 77,\n",
       " '*': 78,\n",
       " 'Z': 79,\n",
       " '+': 80}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_idx_mapping = uniqueCharsByMostCommon(data)\n",
    "print(char_to_idx_mapping['p'])\n",
    "char_to_idx_mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWRTransformer(nn.Module):\n",
    "    def __init__(self, input_width, input_height, char_to_idx_mapping):\n",
    "        super(HWRTransformer, self).__init__()\n",
    "        # create unique char to idx mapping\n",
    "        self.char_to_idx_mapping = char_to_idx_mapping\n",
    "        self.idx_to_char_mapping = {value: key for key, value in char_to_idx_mapping.items()}\n",
    "        # print(self.idx_to_char_mapping)\n",
    "\n",
    "        # convolutional block (5 convolutions)\n",
    "        # first convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3))\n",
    "        width = input_width - 2\n",
    "        height = input_height - 2\n",
    "        self.leakyRelu = nn.LeakyReLU()     # reuse in later layers\n",
    "        self.maxPool = nn.MaxPool2d((2,2))  # reuse in later layers\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm1 = nn.LayerNorm(normalized_shape = [8, height, width])\n",
    "        self.dropout = nn.Dropout(0.2)      # reuse in later layers\n",
    "\n",
    "        # second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # after maxpool\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm2 = nn.LayerNorm(normalized_shape = [16, height, width])\n",
    "\n",
    "\n",
    "        # third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # after maxpool\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm3 = nn.LayerNorm(normalized_shape = [32, height, width])\n",
    "\n",
    "        # forth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # no maxpool\n",
    "        self.layerNorm4 = nn.LayerNorm(normalized_shape = [64, height, width])\n",
    "\n",
    "        # fifth convolutional layer (kernel size to better match shape of character)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (4, 2))\n",
    "        width -= 1\n",
    "        height -= 3\n",
    "        # no maxpool\n",
    "        self.layerNorm5 = nn.LayerNorm(normalized_shape = [128, height, width])\n",
    "\n",
    "        # following is convolution with width 1 which is used to flatten the current output\n",
    "        self.flattenConv = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (height, 1))\n",
    "        self.layerNorm6 = nn.LayerNorm(normalized_shape = [128, 1, width])\n",
    "\n",
    "        # dense layer to upscale from 128 to 256\n",
    "        self.dense1 = nn.Linear(in_features = 128, out_features = 256)\n",
    "        # sinusoidal positional encoding is added to the output of the dense layer\n",
    "        self.pos_encoding = SinPosEncoding(dimensionality = 256)\n",
    "\n",
    "        # transformer encoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
    "        self.trans_encoder1 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder2 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder3 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder4 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "\n",
    "        # Here starts: decoder\n",
    "        # character embedding (dim rule of thumb -> 4th sqrt of nr_embeddings: for ~80 = 3)\n",
    "        self.char_embedding = nn.Embedding(len(self.char_to_idx_mapping), 3)\n",
    "\n",
    "        # transformer decoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
    "        self.trans_decoder1 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder2 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder3 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder4 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        \n",
    "    # character to index for lookup in char embedding table\n",
    "    def charToIndex(self, char):\n",
    "        return self.unique_chars.index(char)\n",
    "\n",
    "    # first forward call: interm_outputs shoudl be a tensor with the embedding of <BOS>\n",
    "    def forward(self, input_img, output_embedding):\n",
    "        # through 5 convolutional layers\n",
    "        # first conv\n",
    "        encoder_out = self.layerNorm1(self.maxPool(self.leakyRelu(self.conv1(input_img))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # second conv\n",
    "        encoder_out = self.layerNorm2(self.maxPool(self.leakyRelu(self.conv2(encoder_out))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # third conv\n",
    "        encoder_out = self.layerNorm3(self.maxPool(self.leakyRelu(self.conv3(encoder_out))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # forth conv\n",
    "        encoder_out = self.layerNorm4(self.leakyRelu(self.conv4(encoder_out)))\n",
    "        # fifth conv\n",
    "        encoder_out = self.layerNorm5(self.leakyRelu(self.conv5(encoder_out)))\n",
    "\n",
    "        # flatten layer\n",
    "        encoder_out = self.layerNorm6(self.leakyRelu(self.flattenConv(encoder_out)))\n",
    "\n",
    "        # dense layer (activation function not mentioned in paper) \n",
    "        # needs reshaped tensor where dims are reversed ((1, 128, 1, x) -> (1, x, 1, 128))\n",
    "        encoder_out = torch.reshape(encoder_out, (encoder_out.size(0), encoder_out.size(3), encoder_out.size(2), encoder_out.size(1)))\n",
    "        encoder_out = self.dense1(encoder_out)\n",
    "\n",
    "        # add sinusodial positional information\n",
    "        # needs reshape (batch, seq_len, 1, 256) -> (seq_len, batch, 256)  \n",
    "        encoder_out = torch.reshape(encoder_out, (encoder_out.size(1), encoder_out.size(0), encoder_out.size(3)))\n",
    "        encoder_out = self.pos_encoding(encoder_out)\n",
    "\n",
    "        # transformer encoder layers\n",
    "        encoder_out = self.trans_encoder1(encoder_out)\n",
    "        encoder_out = self.trans_encoder2(encoder_out)\n",
    "        encoder_out = self.trans_encoder3(encoder_out)\n",
    "        encoder_out = self.trans_encoder4(encoder_out)\n",
    "\n",
    "        # encoder output for CTC Loss\n",
    "        output1 = encoder_out\n",
    "\n",
    "        # add sinusodial positional information again\n",
    "        encoder_out = self.pos_encoding(encoder_out)\n",
    "\n",
    "        # target sequence shifted right\n",
    "        decoder_in = self.char_embedding(output_embedding)\n",
    "        print(decoder_in.shape)\n",
    "        print(encoder_out.shape)\n",
    "        \n",
    "        # input encoder output and predicted chars into decoder (TODO: teacher forcing)\n",
    "        decoder_out = self.trans_decoder1(encoder_out, decoder_in)\n",
    "        decoder_out = self.trans_decoder2(encoder_out, decoder_in)\n",
    "        decoder_out = self.trans_decoder3(encoder_out, decoder_in)\n",
    "        decoder_out = self.trans_decoder4(encoder_out, decoder_in)\n",
    "\n",
    "        return output1, decoder_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize and pad images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizes to largest width in batch x 128, keeping aspect ratio and padding image\n",
    "def resizeBatch(images, image_width, image_height):\n",
    "    resized_batch = torch.empty((images.size(0), 1, image_height, image_width), dtype = torch.float32)\n",
    "    resize_transform = tv.transforms.Resize((image_height, image_width), antialias = True)\n",
    "\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        resized = resize_transform(image)\n",
    "        resized_batch[idx] = resized\n",
    "\n",
    "    return resized_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said \"Nonsense.\" As usual, the optimists have been\n",
      "torch.Size([52, 1, 3])\n",
      "torch.Size([277, 1, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (52x3 and 256x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# print(resized_batch.size())\u001b[39;00m\n\u001b[1;32m     18\u001b[0m plt\u001b[39m.\u001b[39mimshow(resized_batch[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :, :], cmap \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m out1, out2 \u001b[39m=\u001b[39m hwr_transformer(resized_batch, test_label_idxs)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(out1\u001b[39m.\u001b[39msize(), out2\u001b[39m.\u001b[39msize())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[36], line 131\u001b[0m, in \u001b[0;36mHWRTransformer.forward\u001b[0;34m(self, input_img, output_embedding)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mprint\u001b[39m(encoder_out\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    130\u001b[0m \u001b[39m# input encoder output and predicted chars into decoder (TODO: teacher forcing)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m decoder_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrans_decoder1(encoder_out, decoder_in)\n\u001b[1;32m    132\u001b[0m decoder_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrans_decoder2(encoder_out, decoder_in)\n\u001b[1;32m    133\u001b[0m decoder_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrans_decoder3(encoder_out, decoder_in)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:717\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m--> 717\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    718\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    720\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:735\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mha_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    734\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 735\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead_attn(x, mem, mem,\n\u001b[1;32m    736\u001b[0m                             attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    737\u001b[0m                             key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    738\u001b[0m                             is_causal\u001b[39m=\u001b[39;49mis_causal,\n\u001b[1;32m    739\u001b[0m                             need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    740\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1206\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1210\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1211\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1212\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1213\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1214\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1215\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:5224\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5223\u001b[0m     \u001b[39massert\u001b[39;00m in_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 5224\u001b[0m     q, k, v \u001b[39m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[1;32m   5225\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5226\u001b[0m     \u001b[39massert\u001b[39;00m q_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:4777\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4775\u001b[0m     b_q, b_kv \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39msplit([E, E \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m])\n\u001b[1;32m   4776\u001b[0m q_proj \u001b[39m=\u001b[39m linear(q, w_q, b_q)\n\u001b[0;32m-> 4777\u001b[0m kv_proj \u001b[39m=\u001b[39m linear(k, w_kv, b_kv)\n\u001b[1;32m   4778\u001b[0m \u001b[39m# reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m kv_proj \u001b[39m=\u001b[39m kv_proj\u001b[39m.\u001b[39munflatten(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, (\u001b[39m2\u001b[39m, E))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (52x3 and 256x512)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAABNCAYAAACMq59FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1vElEQVR4nO2deXRUVbb/vzUPSSqVpJJURpJASAiZSIAQmUTCLDIIDYgtYis+Wtq20W7btlt0rX4Pn/Tq19rSttoq2Ir4EEGZwhDCGAghEIaQkHlOVWWqVKXm4fz+4HfPS8lMyATns1YW5N6Te8+5Z9pnn7334RFCCBgMBoPBYDAGEPz+zgCDwWAwGAzGT2ECCoPBYDAYjAEHE1AYDAaDwWAMOJiAwmAwGAwGY8DBBBQGg8FgMBgDDiagMBgMBoPBGHAwAYXBYDAYDMaAgwkoDAaDwWAwBhxMQGEwGAwGgzHgYAIKg8FgMBiMAUe/CigbN25EVFQUpFIpMjIycObMmf7MDoPBYDAYjAFCvwko3377LdauXYt169bh3LlzSElJwYwZM6DT6forSwwGg8FgMAYIvP46LDAjIwNjxozBhx9+CABwu92IiIjAr371K/z+97/vjywxGAwGg8EYIAj746V2ux2FhYV444036DU+n4+srCycOnXquvQ2mw02m43+7na70d7ejoCAAPB4vD7JM4PBYDAYjJ5BCIHRaERoaCj4/Ftv4vSLgNLa2gqXy4Xg4GCP68HBwSgtLb0u/fr16/HOO+/0VfYYDAaDwWD0IvX19QgPD79lmn4RUO6WN954A2vXrqW/d3Z2IjIyEvX19VAoFP2YM8ZgghCCzs5ObNiwAQaDAevXr0dXVxe+/fZbjBo1CpMmTervLD60WK1WNDQ0wG63IzIyEt7e3v2dJQaD0QsYDAZERETAx8fntmn7RUBRqVQQCATQarUe17VaLdRq9XXpJRIJJBLJddcVCgUTUP4/LpcLhBAIhYNC5uw3JBIJgoOD0dLSAm9vbxQVFaG5uRmrVq1ibamfcLvdOHXqFK5evYrw8HA0NDRgzpw5kMlk/Z21BwqXy4WamhqEhoYO6G/b2tqKiooKjBkzBgKBoNff53K5kJeXh4SEBAQEBPT6+xjXuBPzjH7x4hGLxUhPT0dOTg695na7kZOTg8zMzP7I0qDG5XJh//792LFjB+x2e39nZ8DicrlgtVpRX18PqVQKoVCIpqYmSKVSJpz0I11dXSgrK8Ps2bMxa9YsqFQqXL58Gf1kv//A0traikOHDkGn08Htdvd3dm5KS0sLKioqPOwOexOXy4XGxkaYTKY+eV9f4nA4+jsLPaLf3IzXrl2LTz/9FJs3b0ZJSQlWr14Nk8mElStX9leWbojT6RxQnZnTlHAQQuByudDe3o7y8nIYjcZ+zN3Ahs/no729HfX19UhPT4dYLIbVakVISAhEIlGf5oUQgvr6ejQ3N/fpewcabrcbBw8ehF6vpyv7zMxMNDU1QaPR9GveXC4XOjo6BlT/7wl2ux1yuRyBgYH9nZWbQghBWVkZQkND++y7O51OtLa2orOz84ESih0OBw4fPgyDwdDfWbln+k1AWbJkCf7yl7/grbfeQmpqKoqKipCdnX2d4WxfQwiB2+2G2+0GIQR5eXk4duzYdYLB3eJwOGC1WuF2u3H8+HHU1dWBEHJHz+TStLW14ccff0RLS4vHfYFAAIVCAbvdjq6urnvO44NOV1cXTpw4gfDwcMydOxfANQ+x4cOH94kquTsdHR346quvcOjQoQdqULxb7HY7Tp48CaVSSbdxJRIJRo4cieLi4j5ZAXZ1deH7779HR0eHx/WWlhZ8++23110frJhMJjQ1NQHAbb0n+gu73Y6LFy/C5XL1WZ90u93QarWQSCQPXF8khAzq9tuvrXTNmjWora2FzWZDfn4+MjIy+jM7tHFynZcQAj8/P5SWlsJgMPSo8Wq1Wvz4449oamrC8ePH6UBxp27STqcT1dXVKCgoQGtrKxWg3G43eDwe/Pz84Ha7B71Krzc5ffo0SktL8eKLL0KtVsPlcsFutyMhIaHX3nmzNlNeXo6WlhYEBgY+1K7yHR0dqKurg0wm8/gO4eHhcLvdfaJhIoSgsrISbW1tHtdbWlpQVlbW6+/vS8xmM6xWa39n46aYzWY0NTVBJBLd0O6wN3C5XAAAi8XyQPVFQgjOnTuHS5cuDVotILOo7AaPx/OYUDhXaIPBgLq6OiQlJd3zs319fdHU1ITTp09DIpHAaDSCEHLbDkEIgdPpREVFBf7973/DarVSQ1in00lXGn5+fpBKpX2uCRhMZGZmYty4cfDy8gKPx4PL5UJGRgZCQkLu+8DE1e2NnutyuXD16lXw+XzExcXd1/cONux2O6xW63Xuhpwxc3FxMdRqNcRica/lQSwWw8/P74Z9klsMPCi4XK4BXR673Q4ejwdfX98+0/KYTCY6lj5IAgqnUecWswDuaM4ZSAxMPV8fwbmdOp1O+jsnpBBC6NaJzWbDgQMHaLp7QSaTITw8HDk5OWhsbER+fj5MJtMdDRY8Hg8mkwl2ux1ms5nmz+12QyQSgcfjQSKRwOFwMAHlFnh7e8PHxwd8Ph88Hg9CoRCPPfZYr7i03moQMJlMKCwsRGRkJMLCwu77uwcTbW1tsNvtUCgUHn2Bx+MhPj4eZrO5140XeTweOjo6aHwmDm9vbwiFQrS3t/fq+/sKq9UKHo/X5/ZWdwM37np5efXZO91uN2w22wPpYOB0OulYNBi3rx5qAcVkMuHzzz9HQUEBFT44+xMOoVCIiIgIlJeX90g1yufzkZaWBpVKhYyMDHR2dmL79u13ZNTqcDjQ3NxMfwwGAxwOB9xuN5xOJ+x2Ox3EpVLpPeeR0bu43W6YzWbs3r0bZ86cQXJycq9qBgYDjY2NEAqFCAwMvM5F3mg0QqvV9mhhcCdw2rT29nYPwZLH48FoND4wAopYLKbjxo0ghKCtre2+TGT3+gyj0YjW1tY+tZGxWq0wGo1QKBSDchK/GS6XCxaLhWqMgTs3KRgoPNRbPDKZDFOnTkVNTQ1MJhPkcjmAa0an3SsyJCQEra2tMBgM9+yOyuPxEBkZiTfffBN8Ph9jxoxBU1PTbTsi56XT3NwMsVgMb29vSKVSep3H44HP56OjowPe3t4swNUAxm634/Dhw9i5cye8vLwwZMiQ/s5Sv0IIgU6ng0qluq7dtrW1Yffu3VCpVPD19e3VfDgcDuj1eg/tCXBN2He5XFTTOVANS+8Ul8tFt4JvRnNzMywWy20jfN4KQgiKi4uhUCgQERFxV5Nie3s7RCIR1aD0xXc3Go0wmUwQi8WDbgK/FU6nEyKRCEKhkP6fE8BuVE5CCBwOB+x2Owgh8PLyuuG359Lx+fzr5sr7zaAWUFwuFzWqqqqqAiEEQUFBSEhIuCMDK4FAgJEjRyIiIgJyuRx8Pp/u0QoEAjidTthsNqp2tFgs95xXbkuBWyXGxMQgJibmjsup1+thMBjg4+NDVbQCgQB8Ph82mw1VVVUYMmTIQ78iH2hw24bcNuGZM2eQlpaGysrKARcU6n7vT99qMOTuNzc3g8/ng8/n073ykpISZGdnY+jQoZg2bVqvt2mn0wmDwXCdIbxIJILb7b7O3flWk6bFYsGZM2eQn5+P+Ph4TJ48udcFrDuls7MTJpPpOkGMg8fjwcvLC/n5+Xd0TsrN4LbJt27diiVLliAyMvK6NnCjtkYIQUtLC7q6umA2m2/qyWOz2VBYWIjAwEDExMT0eFubi7fC5/M98vXTPBJC0NDQgP3798Pf3x/JycmwWq0IDg6+I9ft2/WH+w1nW8PZ85hMJmi1WrS2tiI2NhZ+fn60rpxOJ5qbm7F//360tbXBbDZj1qxZGDt2LAQCAc27VqtFcXExampqEBISgqCgIAwfPhze3t69IkgOagFFo9Fg//79yM/PR1RUFOLi4pCTk4MFCxbcMODbjToFZ2AKXFtJcfe7/99gMMBkMvVZ4KDucOpnvV4PuVwOqVQKq9VKBxmXywWdTofGxkakp6d72NAM9hXfg4LD4cDBgwdx+vRpPPXUU9i9ezf1Wulto7UbtQW9Xo+ioiJER0dTLY7ZbMZXX32FyMhIjB8/3iMM9c3yeLPrLpcLJSUl+Pbbb6FUKrFkyRKoVCoQQq6LYCqRSCCTyejgf+bMGZSVleGRRx7BI4880icCN5/Pp/GOnE4nXURw9hqcbYrT6cS5c+dw+fJlzJw5E2FhYbT8XJn379+PnJwcGAwGnD17Fs3NzVi4cOF9jT1yu4nObDbj8uXLCAgIQFRUlMcEY7Vab7mNERoaCr1e32PNxdChQxEREYEtW7Zg/vz5iImJgVAoRGdnJ/Lz81FWVoaFCxciNDSUChic15bVasVnn32G1NRUJCQkIDAwED4+PlTDfeXKFWzatAmTJk2CSCRCeHh4jyJom0wmKJVK+Pr6wu12w+VyQavVwmazISwsDBKJhAqqmzZtQnFxMZKTk1FVVYWmpiYsWLAAKpXKoz7cbjdaW1thNBoxZMgQ8Pl8HD16FFKpFGPHjqUCg0gkgs1mQ11dHTQaDTQaDUaOHIkRI0b0eLue2xpVKpWorKzE6dOnodPpIJFIkJeXhwULFmDIkCGw2Wy4ePEiiouLER4ejnHjxqG4uBh79+5FTEwM/Pz8QAjB1atXsXv3bkRERCA1NRUOhwOHDh1Ca2srpkyZQpUC58+fh0gkwsiRI0EIgV6vx6VLl+B0OpGenn5XRtqDWkA5fPgwDhw4gIyMDCxevBhhYWGIi4uDRqNBV1cXmpqaUFRUBLVaDT6fD41Gg0cffRReXl7XNWiRSEQlaOBaAxMKhfRaXxptdYcQAoPBgMbGRkilUg9BietMFy9ehL+/P8LCwqDX63H27Fk0NjZi6NChGDt2LKRS6V1Pgk6nkzZwgUBwQ8O6+zW5cga/nFD4U+1Xd+PlwaCC7T4BmEwmnDp1CufPn6cTdU1NDdLT06mBM3BtgqusrKTG1PernGfPnkVHRweysrLohFNSUoITJ06go6MDarUaEokEFRUVOHHiBDIzM1FfX49ly5ZBLpfDbDbjyy+/xKOPPooRI0bQfGm1WhQUFODRRx/12J5xu924cuUK/uu//gvt7e2YOHEiNm7ciEceeQR6vR6LFi2iQgohBE1NTaioqMB3332H+vp6BAQE4Mknn8TQoUP7zOCbs+PiJnLuh1sEcLYbVVVV2LlzJ1WBP/3005DJZHA6nThw4AB27dqFUaNG4d1334VKpYLFYsG+fftw+PBhLFq0qMflsdvtOHbsGEpLSyEQCBAcHIyYmBjEx8fTycxms2HPnj2orKwEn89HQkICJkyYAB8fH9hsNigUCvB4vJsKIGKxGKNGjaK/c9+Bm1Bra2vR3t6OiIgIRERE3FQwkMvlWLx4MVpaWmCxWGA0GqHX6/H999+jpqYGSqUSe/fuRVxcHCZNmkS11/X19TAajRAKhSguLkZJSQkCAgKQlpaG1NRUSCQSeHt7Qy6XY+TIkXRs7ylKpRIikQh2ux25ubk4fPgwlEolRowYgccffxwOhwM7d+5EUFAQVqxYAYVCgQMHDuDChQvg8XgeWyhutxt2ux3Z2dmorKzEa6+9RuePixcvIjY2FkqlEmfOnEFVVRVaWlrQ0tKC6Oho1NbW4sCBAxg/fjyWLVvWowjXNpsNVqsVNTU1+OGHHzB27FgsW7YMYrEYW7duxb59+/CLX/wCeXl5WLduHWbNmoW0tDRUVVXh3LlzSEtLg0Qigd1uR01NDd59910EBwfj6aefhre3NwwGA7y8vJCbm4v09HSoVCoAQGVlJaxWK6KiotDS0oLc3Fy0t7ejrq4OFy9exKJFi+64DINaQPHx8UFwcDDmzJlDpejAwEAUFxfj/PnzOHjwIBQKBfR6PY4cOQJCCGJiYjB8+HDasVwuFxwOB4RCIQ3GJhKJIBAIaCfmNBb9Yf3O4/GoJK5Wq6kKlEOj0aC6uhopKSk0sFRgYCBGjx6NvLw8yGQypKWl3fEKw+FwoKGhAceOHQNwTTjx9vbGuHHjEBwcTI0Jy8rK0N7eDj6fj+TkZISGht71KoYQgq6uLpSWlqK5uRlarRYWiwULFy5EWFgYXC4XGhoaUFFRAYVCAYPBgLi4OERERNzVe/qK7toKm82Grq4unDx5Enl5eVixYgUiIiJQVFSEhoYGPP/883QbEQBqa2vx85//HGFhYfjggw88Vuc9ob29nW5bctsoXORMiURCJ02ZTIZhw4Zh5MiR6OzspKsczm7GYDB49Jv29nbk5eUhLi4OsbGxAK4JJ0VFRfjwww8RHR2N//zP/4RIJML//M//4ODBg9fVG4/Ho1unAoEAixcvRnx8fJ/bUTkcDuh0Oly+fBn79+9HVFQUQkJCIBAIIBQKYbFYYDKZkJubi8TERMhkMmRnZ9PzgrhouOXl5XjuuecQFRUFk8mEqqoqtLW1ITk5uceTqNPpxMmTJ7F9+3bExsairq4OJ0+ehNVqxfz58zF//nx4e3ujubkZGo0GzzzzDADgu+++w8WLF7Fq1Spqa5GdnQ2FQoGoqCgoFAoIhULI5XKP9gCALn4KCwsRGxtL4zcpFApYLBZMnDgR06ZNo1tYhBCUl5dDJpNR4SUkJIR+47KyMjQ2NmL16tWIjo6mmmEOu92Ojo4OKJVKTJ8+HcnJyXR7XSwW0/E3JCQEPj4+OHPmDJKSknr8bbkFkc1mQ3FxMU6cOIH58+dDLpfjX//6F5KSkmA0GlFRUYHnnnsO/v7+0Ol00Ol0MBqN1Halu3BbW1uLI0eOYPz48XRuUavVqKyshEAggEAgwJAhQ/DNN9+gpKQEsbGxUKvViIqKglgsxtGjR+kW4b3CLW6zs7MRGRlJtxvdbjfS0tKwefNmLF68mHovtba24scff0RQUBAWLFiA+Ph4Kpzn5OSguroaQqEQmzdvBiEEFosFEokEc+bM8eizZrMZra2t6Orqwp49e+Dl5YWnnnoK586dw5dffonIyMg7LsOgFlCSkpKQm5uLixcvIjIyEnK5HAqFAkFBQfj666+RkpKCKVOmwM/PDxKJBPv27aPeLpzKtqamBgqFAl5eXtT4x2g0QqPRIDg4GDKZDHw+H1KptF8O2HK5XOjs7IRUKkViYiL2799PYzNUVlbi66+/RmJiIgIDA/GHP/wBxcXFWL16NaKioqDX61FSUoKkpCS6zygUCuF2u+l5Fx0dHYiPj6fRVG02G7755htYLBasXLkScrkczc3NaG1thUqlwqVLl5CTk4PY2FhER0fj8uXL2LJlC+bNm4f4+HiPlSifz0dnZycaGxvh5+cHtVrtIcS43W66Wlm6dCkSEhLQ0NBAB9L8/HxUVVVh2LBhUKlUuHDhAqqrq7FixYoBaWvDxT3hOvShQ4dw8eJFBAUFYceOHQgODkZ5eTlcLhckEglcLheEQiF4PB66uroQEREBhUKBb775Bi+++CJkMlmPhGLOK4PzTiCEwG63o6mpCTqdDiNGjKB2H9HR0Xjsscdw5MgRTJgwga4GxWIxEhMT4e/vD+D/vNxMJhPa29thMpmoZqurqwtbtmxBS0sLXn/9dRoMb8KECdi7d+91RwrweDyEhoYiLCwM8+bNo548fa0pczgcaGlpQX19PSorKxEXF4fExETMmzcPPj4+0Ol0uHDhAq5cuYLXXnsNer0eWq0WGo2GCjK+vr6oqKjAunXrkJaWBpfLBS8vL8yZMwfJyck9Ko/T6cTx48exZ88erFixAomJiWhra6OClUajwZkzZzB58mTU1NTAz88PKpUKIpEI8+bNw5tvvonz58/DYrFAq9XixIkTAK5tw3R1dcHhcMDLywvDhg1DZmYmgoKCAFzT/n388ceoqqqCUChEZmYmfv3rX0OtVuPs2bP417/+BYvFgqVLl0IkEsHhcGDPnj1wu9349a9/7dHXOW9Ih8OBwsJCREdHw8fHx8M+hxuXWltb4ePjA5lMRo0wu8eokkqlmDdvHrZt20brqyd4eXmhq6sLhYWF+Pjjj/HMM88gOTkZBoMBIpEInZ2dOH36NIRCIdRqNcxmMxobG2EwGKDX62Gz2eBwOKimyWw249ChQzCbzZgyZQosFgvKy8uRk5OD9PR0eHt7g8fjUc3SypUrER4ejtraWjQ1NUGpVGL27NkYPnx4j8olkUjgdDrR2dmJcePG4ciRI4iNjUVUVBT18LFarVQ4TU5ORnp6OoKDg6lGjjOabW9vh1AoxOjRo+FwOBAaGoqYmBhERUVRLYtEIgGfz4fFYsHZs2fh7++PlpYWPP7441AoFMjIyEBOTg6OHz9+x2UY1AJKYGAg5syZg23btsHLywtZWVkghNCB87HHHkNwcDB0Oh1aWlroRM9N1l1dXdi3bx+Sk5Np5MqysjJ66N6jjz5KVwKcJNnXcHt/3JkxcrkcjY2NaGhowLZt2yAUCvHII4+gtLQUgYGBmDZtGpqamlBSUoKUlBQcOnQIWq0WXV1d6OjoQGpqKjQaDb777juIRCJIpVIcOXIEf/jDHxAYGAiXywWxWAytVguXywWpVIohQ4ZAKpVCp9Phgw8+wNChQzFmzBiIRCKYzWZcuHABLS0tiImJQUFBAcLCwhAcHIy6ujrk5ubCaDTSlVN8fLxH+Thhi1s5cMJkQUEBPvvsMyxduhSjR4+mXlAFBQWw2+0DUkABQCdvTs356KOPQiQSQSaTwW63o7KyEmq1mnZ8bvBVq9WYNGkSoqOjIZfLYbPZ6J57TygpKYHJZKL5MBgMyM/Ph8PhgFwupytmzrOsubkZ27dvh0KhQEJCAp0EExMTqZBjsVhQWlqKmpoaatPA4/FQU1ODgoICPP3009S2gMfjQSwWo6KiAsuWLbtum0Mmk0EqldIt1f7YwuNiC02bNg1r1qyBr68vJBIJRCIRFSp37doFmUyGgIAA2O122O12NDY2YtSoUeDz+YiPj0dISAiioqLg7++PUaNGISkpCX5+fj3e2tFoNNi6dSsmTJiAlJQUiMViREREgM/nIzo6Gvv374dGo6FjH3ePa1fJyckoKiqCl5cXrFYr5s6di8zMTDqBcWcO7dy5E3V1dfjlL38JgUCA0tJSWCwWLFu2DDk5OeDxeHQMyszMRFVVFb7//nuPcVKpVCIvLw9Wq/W6rb+AgAAsXboUX375JXx8fDBjxgyPNFycJ+7bc22Co/vWVHJyMoqLi7Fnzx6EhYX1SOvm5eUFg8GAQ4cOITo6GpmZmRAKhTR8g1gsRmtrK+RyOdxuNzo7O1FbWws+n081OZyQVFlZiYKCAly6dAk2mw0nT55Ee3s7GhsbMXHiRIwdO5a2ca69+/r6Yty4ccjMzKQLO05r1BPCwsLw8ssv4/Tp0/jwww8RFxcHPz8/6PV67N27F0lJSVAqlYiNjUViYiKOHTtGhW5OM2iz2eDt7Y3w8HAIBAKYzWZMnz4d3t7esFqtKC8vR3NzM7y9vfHYY49BIpFg/PjxOHjwIHbs2IHVq1cjKCgIPB4PMpkMCQkJ2Ldv3x2XYVALKHw+H1OnToVcLsfevXvh5eWF1NRUum/KqdAlEgkiIiJgNpvR3NxMJ0mn04n29nY0NDTQ/fBPPvkEEokEzz77LORyOcrKytDR0QGn09kvQdAsFgsuXLgAi8VCBYPDhw/j8uXL8PLywvPPPw9/f3+UlJRApVLhmWeegUajQWBgINUEORwOnD17FmVlZZBIJDh+/DiSkpIwbtw46PV6FBQUoLm5GUFBQRCJRFi4cCH+/e9/Y+PGjVi0aBG1Pairq0NDQwOSk5Nx9OhR1NfXo7W1FWPGjEFKSgocDgfOnDmDxMREWCwWHD16FKmpqYiJicHHH3+MPXv2IDY21uM7pqWlITs7G8eOHcOsWbPoSunEiRPQaDTo6OiAVqtFS0sLTp48idjY2D4LgX23uN1ulJeXY9++fbDb7VixYgViY2OpLZPRaERBQQHmzJmD+Ph4quolhECpVCIjIwO5ubkIDw9HYmJij/PDaSgKCwthNpthMBiwc+dOmM1mum3h7+/vMWDOnDkTJpMJ27Ztw6pVqyASiVBTUwPgmrBss9mooSh3+CK3suW83jiDwa6uLly9ehUFBQXg8/nw8vKiHnJc/lQqFd1m7S8bI+4067CwMAQGBtKVu8vlQkJCAo4cOQKdToc33ngDQqEQvr6+8Pf3p0K8QCDAhAkTkJubi5qaGiQkJEAul6O1tRVmsxl8Pp9qWu6F6upqEEIwZcoUD3sybiuhtLQUCxYsgNvthsFgQEpKCm1XPB4Pw4cPp3nR6XTw8/NDQECAhzZLpVLBx8cHV65coWXKz8+HUqnE1KlTqRbJYrFAKpVCJBJhxIgR2LZtG8xmM4D/81S8kREu55UzcuRIzJ8/H9999x3a29sxf/58akBstVrR1dWFESNGUA0QJ5S4XC5UVVWBx+PBx8cHDocDo0aNwqZNm/DVV19Ru4h7ISgoCIGBgaiursazzz4LX19fqn10u93w8vJCZGQkdu3aha1bt8LlciEyMhJPPfUUvL29sWfPHmg0GrrgGz16NDIzM/HFF18gNzcXjz32GObNmwd/f3+PbbSgoCCEhIRg+/bt8Pf3x/Dhw6nRePfor/eKWCxGWloa/bYulwunTp1CS0sLxo0bh5kzZ0Imk0EikeD3v/89rl69SjVDYrEYAQEBiI6OhkqlgtvthkKhwOHDh/HJJ5/A398ffn5+GDZsGKKjo+kCEwCGDRuG119/HWazGcnJyQBA28WIESNQWlp6x2W4awHl2LFj2LBhAwoLC9Hc3IwdO3Zg/vz59D4hBOvWrcOnn34KvV6P8ePH46OPPqL71MC1/etf/epX2LVrF/h8Pp588km8//77d93A+Hw+JBIJ9djZvXs3HA4HoqOj6Z7hmDFjqAFmQEAADhw4gISEBPj7+6O+vh5Op5MeTpWXl4fq6mq88847iIqKAiEEAQEBaGtr6/Pohhzt7e1ob29HSkoKQkJCsHjxYqjVanR2dmLy5MkICwuD0WhEeXk51Go11Go1wsPDqUGdv78/1Go1CCGoqKjAZ599hsWLF2PMmDEQCAR0Bezt7U07RFBQEFatWoVTp07hyy+/xOjRo7FgwQJ4e3vD5XLhypUrtLENHTqUWrprtVpkZ2ejubkZMpkMCxYswLBhw8Dj8aBQKK47LdTtdmPo0KF45plnsHv3bggEAkyaNInmNSAgAEePHsXJkyfppJ2VlTUgI2E6nU6Ul5fj73//O5KSkvCzn/2Meodxql/OfW/IkCEexxVwnlpxcXHw8vLCli1b0NjYiJUrV1J1+70yfvx4HD9+HF988QX0ej1Gjx6NF154ARs2bMCJEycQERFBDQ8tFgtsNhuGDh1KNYnLly+HWq1GeXk56uvrUVJSgh9++AGzZ8+GRCLB5cuXMX78ePj6+iIsLIxO6G1tbXC5XIiIiMCCBQtQVlaGqqoqREdHQ6FQ0O1SkUgEo9FIv8PdumI2NDRAq9UiNTX1ngUAb29vhIWF0S3Q7gJUfHw8bW9RUVF0y3f06NHUtoeztXjzzTfx448/oqCgAGVlZVAqlQgKCkJsbCxUKtU958/hcMDpdNJo0gKBAEajEUVFRSgrK8O0adMQFhZG7Z44rxODwYBLly6hqqoKkyZNQk5ODj1U9Kfus923gLl7QUFB8PX1hVKpRFpaGhVMuIVfcXExhg0b5uGhxG31dMftdiMvLw8ajYZqt5OTk3H8+HHYbDa8+OKLEIlEEIvFUCgUGDt2LHx8fGi+ONra2lBUVASHw4GLFy9Cr9fTbfrMzEykpKTc0/f19vamwl9qaiqdTKOiojBp0iTweDxMnjwZDQ0N0Ol0eOKJJzBy5EgIhUI89dRTiI2NhU6nQ0REBBISEugWzttvv02fz7Wh7q7MYrEYK1euxL59+3D06FHk5eVBIpEgLi4O8fHxCA0N7dFYxy0Kxo8fj1GjRtGQGUKhkEZJ5uzAgoODERQUdJ3XX/d2smjRIsydO5e2H07TxWnrOCQSCUaPHu2RF054zcjIQExMDP7617/eURnuWkAxmUxISUnBc889h4ULF153/7333sMHH3yAzZs3Izo6Gn/6058wY8YMXLlyhe5rLV++HM3NzTh48CAcDgdWrlyJVatWYcuWLXeVl4qKCohEIohEIigUCqhUKuzatQsvvfQShg0bhu3bt0MqlaKsrAxisRgvvPACPv30U/zjH/9Aamoqddvl7FLkcjnkcjkNfsbj8WAwGNDc3HzT2AG9TVtbG3Q6HWbPng2xWAypVIrZs2cD8Gw8crkcly5dwqVLl2jk27a2NmRlZVFbhtraWoSGhiIlJYU2QK1WS+MfOJ1ONDU1oaOjA11dXRAKhZgwYQKOHz8OpVKJadOmITk5GWVlZZg7dy4dEGpra+Hv708Dx1VXV+Pll1+mwonFYkFlZSXGjx/vMUjbbDZoNBr4+/tj2rRp2LFjB3Q6HWbOnAm73Q4fHx/87Gc/g7e3N6KioqBUKm+o9nS5XLDZbHA6nT2227hXKioqsH37dmRlZSErK4uejMoZ+fF4PDQ0NFCXRi7mA2eMLJPJUFpaCpvNRi38IyIisGzZsh5pFWJiYvD0009Dq9ViypQpSExMBJ/Px8svvwyj0YiamhpcunQJ/v7+1D2VU9Vzrp0LFy7Eli1bcP78eYSFheGJJ57AxIkTkZiYiG3btuH777/Hk08+icDAQPzxj39EXl4ebDYbxo8fj6ioKADAypUrcfnyZZw6dQqTJ0+mAkp4eDhCQ0MhFos9tCt3gsViwffff4/GxkaMHDnyngUAhUKBt956i9qacd9bIBAgPj4ev/nNb9Da2kq1IHK5HKtXr6aTDJc2MjISq1evxrPPPksnIW7F3BNDzuHDh0MqleKzzz5DcnIyhEIhdDodoqKiMHfuXHqWlE6nQ2lpKU6cOAGJREINoRcuXAiFQoGvvvqKGsd294rjJqlx48YhLi6OCs8LFiygk9WECROQmZlJvw8Xf2ru3Lm0Lnk8HrKyspCWluaxmOPz+Rg/fjwsFgsaGxtx9OhRui0A/J/XW0BAADZs2IDAwEBqz8A9l8/nIyMjA0lJSbDZbJg9ezY0Gg1aW1upfcu9wmkOp0+f7rGlFBgYiBUrVtD28Mc//pGWh7smk8kwY8aMGz73djaLXl5eSExMxIgRI+B0OmmkcolE4lH+nsDVrY+Pj0fogBulu904w7X922093+w5nNv+3SgieKQHeiQej+ehQSGEIDQ0FK+++ipee+01ANeCAwUHB2PTpk1YunQpSkpKkJCQgIKCAiplZWdnY/bs2WhoaEBoaOht32swGODr64vly5fD6XSCz+cjICAAAoEA6enpmDlzJurr6/HFF1+gq6sLqampmD17NoKCglBcXIzvvvsOarUac+fORX5+PrRaLdauXYv6+nqsX78eJpOJGgt1dXUhODgYpaWlmDdvXq+efHsjKioqsHnzZjz33HNUE8I1Jm4/llvNfPjhh9Dr9QgPD8eYMWMwffp0qsotKirC559/jqVLl2Ls2LH0OVeuXMHly5exZMkStLS04G9/+xva2trQ2dkJHx8feHl5QaVSYe7cuRg1ahRqa2uxd+9eXLp0Cb6+vkhISKCdTCQSobCwECqVigZn4vGunSO0f/9+TJgwwSNC5e7du7F161ZYrVaIRCIaWOjZZ5+FSqXCDz/8AOCaMV9MTAyGDBmC8PBwKJVKj05w6dIlHDp0CDU1NRgzZgwWLVrUpyH/HQ4HPv/8cwwfPhyjR4+mQi63KgWurSK3bt2K/fv34/3334efnx8sFgvq6urQ1tYGPp+PqqoqGjGYx+Nh1qxZGDFiRI/zx6mLu0/gXLfnDhQTiURwuVxUEBcKhZBIJJBKpdSwltu+4QI/cROV3W6Hr68vff6NtCCc+yU32XBwkVrvNtCTxWLBDz/8gAMHDuCpp57C1KlTB4UL+r1ACEFjYyPOnj0L4Np2THdPI47a2lq89dZbePzxxxEZGQm1Wo3g4GCIxWLYbDb85S9/gU6nwzvvvOMR8LH7e+4kfhLnTNDU1EQdCe62PN0DYrLzwx4uuPm7s7Pztm7U91VAqaqqwtChQ3H+/HmkpqbSdJMnT0Zqairef/99fP7553j11VfR0dFB7zudTkilUmzbtg0LFiy47j02m80jSJrBYEBERAROnDiBkJAQGteEUzlxK1aLxUKNAbnOyHUuPp8PoVBIz6aQyWSw2WzQarW4evUqtFotjUqrVCrp6ryv7R+4znwrF15uItTr9aivr4eXlxfCwsIglUo9pH2Hw+ERe4P7W25Q4tz+uL/hviOnperupmq1WqkBlUQiuSMJ/KfodDpotVqoVCpqtGw0GhEcHIzQ0FB6RgZ3oCMXlOmnEnhtbS02b96MkpISPPPMM/2yDdTa2gpvb2+IxWL6nbr/a7fb8fnnn6O4uBh//vOf6VZBb4eKfhCxWq04fvw4qqqqkJaWRl1/H3Z0Oh3+9re/4eWXX4afnx/t61z7amhogNvtvuvw8wzG/eRuBJT7aiTLhYQODg72uB4cHEzvaTSa6/bVhUIh/P39rwspzbF+/Xq88847111PSkq6ZQFvpkrqLrF3FzgkEgkiIyPvyk+7t+H27m6XRiQSITAw8JYRK2+0PdJ9ABMIBLcNv87n8yEWi++LF01QUNAtbSxEIhF8fHxuq1WLjIzEyy+/DEIIFApFv6zIuCBFnKbgp6tQsViMJ554ApMnT6YnKjPuDaFQiPT0dEyYMIEJJt3gFhh2u50eH9Cdnpyvw2D0B4NilHzjjTfQ2dlJf+rr6/s7S4wBBI/Hg1KpvC8unfcjL90nhu7/Dw0NxYgRI5hw0kO4BQ0TTjyx2+10y7u/+wGDcT+4ryOlWq0GcM3wsjtarZbeU6vV0Ol0Hvc5d18uzU+RSCRQKBQePwwGg8H4P0wmEwQCAd0+ZDAGO/e1FUdHR0OtViMnJ4de4wJDca7AmZmZ0Ov1KCwspGkOHz4Mt9uNjIyM+5kdBoPBeGjghBLOzbencTQYjP7mrm1Qurq6UFFRQX+vrq5GUVER/P39ERkZiVdeeQV//vOfaSj0P/3pTwgNDaWGtCNGjMDMmTPxwgsv4J///CccDgfWrFmDpUuX3pEHD4PBYDCup7vB9WA5WJPBuBV3LaCcPXsWU6ZMob+vXbsWALBixQps2rQJv/vd72AymbBq1Sro9XpMmDAB2dnZHm6fX3/9NdasWYOpU6fSQG0ffPDBHeeBWxkYDIa7zT6DwWA8kIjFYiiVSnR2dt6XYxIYjN6Am7fvRMPXIzfj/oJzZ2YwGAwGgzH4qK+vv61n2aA8i4c7WbWurs7jNEzGwIaLX1NfX88MnQcRrN4GH6zOBicPQ70RQmA0Gu/IpGNQCiicMZivr+8DW4kPMswTa3DC6m3wwepscPKg19udKhaYLxqDwWAwGIwBBxNQGAwGg8FgDDgGpYAikUiwbt26Pj8Xh9EzWL0NTli9DT5YnQ1OWL15Mii9eBgMBoPBYDzYDEoNCoPBYDAYjAcbJqAwGAwGg8EYcDABhcFgMBgMxoCDCSgMBoPBYDAGHINSQNm4cSOioqIglUqRkZGBM2fO9HeWHlrefvtt8Hg8j5/4+Hh632q14qWXXkJAQAC8vb3x5JNPQqvVejyjrq4Oc+bMgVwuR1BQEH7729/C6XT2dVEeaI4dO4a5c+ciNDQUPB4PO3fu9LhPCMFbb72FkJAQyGQyZGVloby83CNNe3s7li9fDoVCAaVSiV/84hfo6urySHPx4kVMnDgRUqkUEREReO+993q7aA8st6uzZ5999rq+N3PmTI80rM76nvXr12PMmDHw8fFBUFAQ5s+fj6tXr3qkuV/j4pEjR5CWlgaJRIJhw4Zh06ZNvV28PmXQCSjffvst1q5di3Xr1uHcuXNISUnBjBkzoNPp+jtrDy0jR45Ec3Mz/Tlx4gS995vf/Aa7du3Ctm3bcPToUTQ1NWHhwoX0vsvlwpw5c2C325GXl4fNmzdj06ZNeOutt/qjKA8sJpMJKSkp2Lhx4w3vv/fee/jggw/wz3/+E/n5+fDy8sKMGTNgtVppmuXLl6O4uBgHDx7E7t27cezYMaxatYreNxgMmD59OoYMGYLCwkJs2LABb7/9Nj755JNeL9+DyO3qDABmzpzp0fe++eYbj/uszvqeo0eP4qWXXsLp06dx8OBBOBwOTJ8+HSaTiaa5H+NidXU15syZgylTpqCoqAivvPIKnn/+eezfv79Py9urkEHG2LFjyUsvvUR/d7lcJDQ0lKxfv74fc/Xwsm7dOpKSknLDe3q9nohEIrJt2zZ6raSkhAAgp06dIoQQsnfvXsLn84lGo6FpPvroI6JQKIjNZuvVvD+sACA7duygv7vdbqJWq8mGDRvoNb1eTyQSCfnmm28IIYRcuXKFACAFBQU0zb59+wiPxyONjY2EEEL+8Y9/ED8/P496e/3110lcXFwvl+jB56d1RgghK1asIPPmzbvp37A6GxjodDoCgBw9epQQcv/Gxd/97ndk5MiRHu9asmQJmTFjRm8Xqc8YVBoUu92OwsJCZGVl0Wt8Ph9ZWVk4depUP+bs4aa8vByhoaGIiYnB8uXLUVdXBwAoLCyEw+HwqK/4+HhERkbS+jp16hSSkpIQHBxM08yYMQMGgwHFxcV9W5CHlOrqamg0Go968vX1RUZGhkc9KZVKjB49mqbJysoCn89Hfn4+TTNp0iSIxWKaZsaMGbh69So6Ojr6qDQPF0eOHEFQUBDi4uKwevVqtLW10XuszgYGnZ2dAP7vkNv7NS6eOnXK4xlcmgdpLhxUAkpraytcLpdHpQFAcHAwNBpNP+Xq4SYjIwObNm1CdnY2PvroI1RXV2PixIkwGo3QaDQQi8VQKpUef9O9vjQazQ3rk7vH6H2473yrfqXRaBAUFORxXygUwt/fn9VlPzFz5kx8+eWXyMnJwX//93/j6NGjmDVrFlwuFwBWZwMBt9uNV155BePHj0diYiIA3Ldx8WZpDAYDLBZLbxSnzxmUpxkzBg6zZs2i/09OTkZGRgaGDBmC//3f/4VMJuvHnDEYDzZLly6l/09KSkJycjKGDh2KI0eOYOrUqf2YMwbHSy+9hMuXL3vY5THunEGlQVGpVBAIBNdZO2u1WqjV6n7KFaM7SqUSw4cPR0VFBdRqNex2O/R6vUea7vWlVqtvWJ/cPUbvw33nW/UrtVp9nSG60+lEe3s7q8sBQkxMDFQqFSoqKgCwOutv1qxZg927dyM3Nxfh4eH0+v0aF2+WRqFQPDCLw0EloIjFYqSnpyMnJ4dec7vdyMnJQWZmZj/mjMHR1dWFyspKhISEID09HSKRyKO+rl69irq6OlpfmZmZuHTpksdAevDgQSgUCiQkJPR5/h9GoqOjoVarPerJYDAgPz/fo570ej0KCwtpmsOHD8PtdiMjI4OmOXbsGBwOB01z8OBBxMXFwc/Pr49K8/DS0NCAtrY2hISEAGB11l8QQrBmzRrs2LEDhw8fRnR0tMf9+zUuZmZmejyDS/NAzYX9baV7t2zdupVIJBKyadMmcuXKFbJq1SqiVCo9rJ0Zfcerr75Kjhw5Qqqrq8nJkydJVlYWUalURKfTEUII+Y//+A8SGRlJDh8+TM6ePUsyMzNJZmYm/Xun00kSExPJ9OnTSVFREcnOziaBgYHkjTfe6K8iPZAYjUZy/vx5cv78eQKA/PWvfyXnz58ntbW1hBBC3n33XaJUKskPP/xALl68SObNm0eio6OJxWKhz5g5cyYZNWoUyc/PJydOnCCxsbFk2bJl9L5eryfBwcHk5z//Obl8+TLZunUrkcvl5OOPP+7z8j4I3KrOjEYjee2118ipU6dIdXU1OXToEElLSyOxsbHEarXSZ7A663tWr15NfH19yZEjR0hzczP9MZvNNM39GBerqqqIXC4nv/3tb0lJSQnZuHEjEQgEJDs7u0/L25sMOgGFEEL+/ve/k8jISCIWi8nYsWPJ6dOn+ztLDy1LliwhISEhRCwWk7CwMLJkyRJSUVFB71ssFvLLX/6S+Pn5EblcThYsWECam5s9nlFTU0NmzZpFZDIZUalU5NVXXyUOh6Ovi/JAk5ubSwBc97NixQpCyDVX4z/96U8kODiYSCQSMnXqVHL16lWPZ7S1tZFly5YRb29volAoyMqVK4nRaPRIc+HCBTJhwgQikUhIWFgYeffdd/uqiA8ct6ozs9lMpk+fTgIDA4lIJCJDhgwhL7zwwnULNVZnfc+N6gwA+eKLL2ia+zUu5ubmktTUVCIWi0lMTIzHOx4EeIQQ0tdaGwaDwWAwGIxbMahsUBgMBoPBYDwcMAGFwWAwGAzGgIMJKAwGg8FgMAYcTEBhMBgMBoMx4GACCoPBYDAYjAEHE1AYDAaDwWAMOJiAwmAwGAwGY8DBBBQGg8FgMBgDDiagMBgMBoPBGHAwAYXBYDAYDMaAgwkoDAaDwWAwBhxMQGEwGAwGgzHg+H+Ba35MMyUm1QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hwr_transformer = HWRTransformer(input_width, INPUT_HEIGHT, char_to_idx_mapping)\n",
    "\n",
    "test_image, test_label = train_set.__getitem__(1)\n",
    "# print(\"Mean of image: %.2f\" % test_image.float().mean().item())\n",
    "\n",
    "# create \"batch\" with single image resize\n",
    "test_image_batch = test_image.unsqueeze(0)\n",
    "print(test_label)\n",
    "test_label_idxs = torch.tensor([[hwr_transformer.char_to_idx_mapping[char]] for char in test_label])\n",
    "# add BOS and EOS tokens at beginning and end of sentence\n",
    "test_label_idxs = torch.cat([torch.tensor([[hwr_transformer.char_to_idx_mapping['<BOS>']]]), test_label_idxs])\n",
    "test_label_idxs = torch.cat([test_label_idxs, torch.tensor([[hwr_transformer.char_to_idx_mapping['<EOS>']]])])\n",
    "\n",
    "# resize\n",
    "resized_batch = resizeBatch(test_image_batch, input_width, INPUT_HEIGHT)\n",
    "# print(resized_batch.size())\n",
    "\n",
    "plt.imshow(resized_batch[0, 0, :, :], cmap = \"gray\")\n",
    "\n",
    "out1, out2 = hwr_transformer(resized_batch, test_label_idxs)\n",
    "print(out1.size(), out2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
