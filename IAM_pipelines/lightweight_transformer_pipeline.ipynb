{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Transformer-based Model for Handwritten Character Recognition \n",
    "(https://hal.science/hal-03685976/file/A_Light_Transformer_Based_Architecture_for_Handwritten_Text_Recognition.pdf)\n",
    "\n",
    "## ***note: Has a CNN backbone***\n",
    "\n",
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture  \n",
    "Build up with a double Transformer architecture:  \n",
    "- Image transformer as encoder: Extracts the visual features\n",
    "- Text transformer as decoder: Language modeling\n",
    "- Encoder: \n",
    "- Decoder: Generates word-sections sequence using visual features and previous predictions\n",
    "\n",
    "### Encoder:  \n",
    "- CNN Backbone (5 convolutions)\n",
    "- Sinusodial position encoding  \n",
    "- 4 layer transformer layer encoder\n",
    "\n",
    "### Decoder: \n",
    "- Takes encoder output and along with sequence of previously predicted characters\n",
    "- Additional loss in the middle of the network to help convergence\n",
    "\n",
    "--------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_image\n",
    "import torchvision as tv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"/home/hkolstee/uniprojects/DATA/HWR/IAM-data/IAM-data/\"\n",
    "TRAIN_TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "INPUT_HEIGHT = 128\n",
    "# input width -> largest width in batch\n",
    "# padded to get to width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_names</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a03-017-07.png</td>\n",
       "      <td>into the pro-communist north and the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a03-017-05.png</td>\n",
       "      <td>to 1958 kept the kingdom in peace, though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a03-017-08.png</td>\n",
       "      <td>pro-western centre and south.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a03-017-02.png</td>\n",
       "      <td>in Phnom Penh indicate that he still regards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a03-017-06.png</td>\n",
       "      <td>at the cost of virtual partition of the country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7453</th>\n",
       "      <td>d06-000-08.png</td>\n",
       "      <td>fears are based upon completely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7454</th>\n",
       "      <td>d06-000-05.png</td>\n",
       "      <td>is worrying them, to find the original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7455</th>\n",
       "      <td>d06-000-09.png</td>\n",
       "      <td>irrational pre-conceived notions - or to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7456</th>\n",
       "      <td>d06-000-02.png</td>\n",
       "      <td>already suggested, not to be silly or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7457</th>\n",
       "      <td>d06-000-00.png</td>\n",
       "      <td>In the first place it is not a great deal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7458 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           img_names                                           labels\n",
       "0     a03-017-07.png             into the pro-communist north and the\n",
       "1     a03-017-05.png        to 1958 kept the kingdom in peace, though\n",
       "2     a03-017-08.png                    pro-western centre and south.\n",
       "3     a03-017-02.png     in Phnom Penh indicate that he still regards\n",
       "4     a03-017-06.png  at the cost of virtual partition of the country\n",
       "...              ...                                              ...\n",
       "7453  d06-000-08.png                  fears are based upon completely\n",
       "7454  d06-000-05.png           is worrying them, to find the original\n",
       "7455  d06-000-09.png         irrational pre-conceived notions - or to\n",
       "7456  d06-000-02.png            already suggested, not to be silly or\n",
       "7457  d06-000-00.png        In the first place it is not a great deal\n",
       "\n",
       "[7458 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_fwf(DATA_PATH + \"iam_lines_gt.txt\", header = None)\n",
    "raw_data = raw_data.values.tolist()\n",
    "\n",
    "data = {'img_names': np.squeeze(raw_data[::2]),\n",
    "        'labels': np.squeeze(raw_data[1::2])}\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Data augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = TRAIN_TEST_SPLIT)\n",
    "\n",
    "# reset indices from current random state\n",
    "train.reset_index(inplace = True)\n",
    "test.reset_index(inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom pytorch dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we need the input/image width we have to resize the images to.  \n",
    "This is the largest image width in the entire batch of images (source paper randomly added/removed new augments each training epoch).   \n",
    "For now we just take the largest width in the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBiggestWidth(data: pd.DataFrame):\n",
    "    biggest_width = 0\n",
    "\n",
    "    for index in range(len(data['img_names'])):\n",
    "        image_path = os.path.join(DATA_PATH, \"img\", data['img_names'][index])\n",
    "        image = read_image(image_path)\n",
    "        \n",
    "        if (image.size(2) > biggest_width):\n",
    "            biggest_width = image.size(2)\n",
    "\n",
    "    return biggest_width\n",
    "\n",
    "def getLongestLabel(data: pd.DataFrame):\n",
    "    longest = 0\n",
    "    \n",
    "    for index in range(len(data['labels'])):\n",
    "        if (len(data['labels'][index]) > longest):\n",
    "            longest = len(data['labels'][index])\n",
    "            \n",
    "    return longest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biggest width needed to pad all images to this width for the input into the encoder.  \n",
    "Longest label needed to pad all labels to this length for the input into the decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "input_width = getBiggestWidth(data)\n",
    "print(input_width)\n",
    "\n",
    "longest_label = getLongestLabel(data)\n",
    "# <BOS> and <EOS> tokens not counted\n",
    "longest_label += 2\n",
    "print(longest_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For character level embedding (decoder input) we find out how many characters are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dict of uniques chars sorted on how common they are in the dataset labels\n",
    "def uniqueCharsByMostCommon(data: pd.DataFrame):\n",
    "    sortedDict = OrderedDict(Counter(''.join(data['labels'].values)).most_common())\n",
    "    newDict = {}\n",
    "    \n",
    "    # first add pad, begin of sentence, and end of sentence tokens\n",
    "    newDict[\"<PAD>\"] = 0\n",
    "    newDict[\"<BOS>\"] = 1\n",
    "    newDict[\"<EOS>\"] = 2\n",
    "    \n",
    "    for idx, char in enumerate(sortedDict):\n",
    "        newDict[char] = idx + 3\n",
    "    \n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandWritingDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, img_width, img_height, char_to_idx_mapping, max_label_size, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.char_to_idx_mapping = char_to_idx_mapping\n",
    "        self.idx_to_char_mapping = {value: key for key, value in self.char_to_idx_mapping.items()}\n",
    "\n",
    "        # change character level strings to embed indices \n",
    "        #   (embedding itself calculated in forward pass)\n",
    "        self.labels_as_idxs = [torch.tensor([[self.char_to_idx_mapping[char]] for char in label]) for label in data['labels']]   \n",
    "        # add <BOS> and <EOS> tokens at begin and end of sentences\n",
    "        self.labels_as_idxs = [torch.cat([torch.tensor([[self.char_to_idx_mapping['<BOS>']]]), label]) for label in self.labels_as_idxs]\n",
    "        self.labels_as_idxs = [torch.cat([label, torch.tensor([[self.char_to_idx_mapping['<EOS>']]])]) for label in self.labels_as_idxs]\n",
    "        \n",
    "        # pad labels to largest label length with <pad> token\n",
    "        # print(self.labels_as_idxs[0].shape)\n",
    "        self.labels_as_idxs = [F.pad(label, (0, 0, 0, max_label_size - label.shape[0]), mode = 'constant', value = self.char_to_idx_mapping['<PAD>']) for label in self.labels_as_idxs]\n",
    "        # print(self.labels_as_idxs[0].shape)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # input image\n",
    "        image_path = os.path.join(DATA_PATH, \"img\", self.data['img_names'][index])\n",
    "        # torchvision read_image call\n",
    "        image = read_image(image_path)\n",
    "        # resize to \n",
    "        \n",
    "        # string label\n",
    "        label = self.labels_as_idxs[index]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        # return length of column\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<BOS>': 1,\n",
       " '<EOS>': 2,\n",
       " ' ': 3,\n",
       " 'e': 4,\n",
       " 't': 5,\n",
       " 'a': 6,\n",
       " 'o': 7,\n",
       " 'n': 8,\n",
       " 'i': 9,\n",
       " 's': 10,\n",
       " 'r': 11,\n",
       " 'h': 12,\n",
       " 'l': 13,\n",
       " 'd': 14,\n",
       " 'c': 15,\n",
       " 'u': 16,\n",
       " 'm': 17,\n",
       " 'f': 18,\n",
       " 'p': 19,\n",
       " 'w': 20,\n",
       " 'g': 21,\n",
       " 'y': 22,\n",
       " 'b': 23,\n",
       " '.': 24,\n",
       " ',': 25,\n",
       " 'v': 26,\n",
       " 'k': 27,\n",
       " \"'\": 28,\n",
       " '\"': 29,\n",
       " '-': 30,\n",
       " 'T': 31,\n",
       " 'I': 32,\n",
       " 'M': 33,\n",
       " 'A': 34,\n",
       " 'S': 35,\n",
       " 'B': 36,\n",
       " 'P': 37,\n",
       " 'H': 38,\n",
       " 'W': 39,\n",
       " 'C': 40,\n",
       " 'N': 41,\n",
       " 'G': 42,\n",
       " 'x': 43,\n",
       " 'R': 44,\n",
       " 'L': 45,\n",
       " 'E': 46,\n",
       " 'D': 47,\n",
       " 'F': 48,\n",
       " '0': 49,\n",
       " '1': 50,\n",
       " 'j': 51,\n",
       " 'O': 52,\n",
       " 'q': 53,\n",
       " '!': 54,\n",
       " 'U': 55,\n",
       " '(': 56,\n",
       " 'K': 57,\n",
       " '?': 58,\n",
       " 'z': 59,\n",
       " '3': 60,\n",
       " ')': 61,\n",
       " '9': 62,\n",
       " ';': 63,\n",
       " 'V': 64,\n",
       " '2': 65,\n",
       " 'J': 66,\n",
       " 'Y': 67,\n",
       " ':': 68,\n",
       " '5': 69,\n",
       " '8': 70,\n",
       " '4': 71,\n",
       " '6': 72,\n",
       " '#': 73,\n",
       " '&': 74,\n",
       " '7': 75,\n",
       " '/': 76,\n",
       " 'Q': 77,\n",
       " 'X': 78,\n",
       " '*': 79,\n",
       " 'Z': 80,\n",
       " '+': 81}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get mapping before splitting dataset\n",
    "char_to_idx_mapping = uniqueCharsByMostCommon(data)\n",
    "\n",
    "train_set = HandWritingDataset(train, input_width, INPUT_HEIGHT, char_to_idx_mapping, longest_label, BATCH_SIZE)\n",
    "test_set = HandWritingDataset(test, input_width, INPUT_HEIGHT, char_to_idx_mapping, longest_label, BATCH_SIZE)\n",
    "\n",
    "char_to_idx_mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinusodial positional encoding  \n",
    "(can be changed to nn.embedding layers if we don't get good results, however that is not exactly sinusodial pos encoding like in the paper I think)\n",
    "\n",
    "<!-- **CHANGED TO NN.EMBEDDING IN MODEL**   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinPosEncoding(nn.Module):\n",
    "    def __init__(self, dimensionality):\n",
    "        super(SinPosEncoding, self).__init__()\n",
    "        self.dims = dimensionality\n",
    "        self.max_len = 1000\n",
    "\n",
    "        # position vector\n",
    "        positions = torch.arange(0, self.max_len).unsqueeze(1)\n",
    "        # calculate added angle for sin/cos\n",
    "        angle = torch.exp(torch.arange(0, self.dims, 1) * (-np.log(10000.0) / self.dims))\n",
    "\n",
    "        # initialize the 2D positional encodings array\n",
    "        pos_encodings = torch.zeros(self.max_len, 1, self.dims)\n",
    "        # calucalte encodings\n",
    "        pos_encodings[:, 0, :] = torch.sin(positions * angle)\n",
    "\n",
    "        # add to buffer for training performance (?)\n",
    "        self.register_buffer('pos_encodings', pos_encodings)\n",
    "\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # print(input.shape)\n",
    "        # print(self.pos_encodings.shape)\n",
    "        # adds the positional encoding elementwise to the tensor (seqlength, batch, embeddims)\n",
    "        input += self.pos_encodings[0:input.size(1)]\n",
    "        # print(\"succes\")\n",
    "\n",
    "        return input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWRTransformer(nn.Module):\n",
    "    def __init__(self, input_width, input_height, nr_char_tokens, output_dims):\n",
    "        super(HWRTransformer, self).__init__()\n",
    "        # convolutional block (5 convolutions)\n",
    "        # first convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3))\n",
    "        width = input_width - 2\n",
    "        height = input_height - 2\n",
    "        self.leakyRelu = nn.LeakyReLU()     # reuse in later layers\n",
    "        self.maxPool = nn.MaxPool2d((2,2))  # reuse in later layers\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm1 = nn.LayerNorm(normalized_shape = [8, height, width])\n",
    "        self.dropout = nn.Dropout(0.2)      # reuse in later layers\n",
    "\n",
    "        # second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # after maxpool\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm2 = nn.LayerNorm(normalized_shape = [16, height, width])\n",
    "\n",
    "\n",
    "        # third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # after maxpool\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm3 = nn.LayerNorm(normalized_shape = [32, height, width])\n",
    "\n",
    "        # forth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # no maxpool\n",
    "        self.layerNorm4 = nn.LayerNorm(normalized_shape = [64, height, width])\n",
    "\n",
    "        # fifth convolutional layer (kernel size to better match shape of character)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (4, 2))\n",
    "        width -= 1\n",
    "        height -= 3\n",
    "        # no maxpool\n",
    "        self.layerNorm5 = nn.LayerNorm(normalized_shape = [128, height, width])\n",
    "\n",
    "        # following is convolution with width 1 which is used to flatten the current output\n",
    "        self.flattenConv = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (height, 1))\n",
    "        self.layerNorm6 = nn.LayerNorm(normalized_shape = [128, 1, width])\n",
    "\n",
    "        # dense layer to upscale from 128 to 256\n",
    "        self.dense1 = nn.Linear(in_features = 128, out_features = 256)\n",
    "        # sinusoidal positional encoding is added to the output of the dense layer\n",
    "        self.encoder_pos_encoding = SinPosEncoding(dimensionality = 256)\n",
    "\n",
    "        # transformer encoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
    "        self.trans_encoder1 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder2 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder3 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder4 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "\n",
    "        # Here starts: decoder\n",
    "        # character embedding (dim rule of thumb -> 4th sqrt of nr_embeddings: for ~80 = 3) \n",
    "        #      NOTE: wrong, appearantly dims (encoder output, target embedding) need to be the same\n",
    "        # <PAD> embedding idx = 0\n",
    "        self.char_embedding = nn.Embedding(nr_char_tokens, 256, padding_idx = 0)\n",
    "        # positional embedding of decoder input sequence\n",
    "        self.decoder_pos_encoding = SinPosEncoding(dimensionality = 256)\n",
    "\n",
    "        # transformer decoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
    "        self.trans_decoder1 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder2 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder3 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder4 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        \n",
    "        # dense layer size of output \n",
    "        self.decoder_out_dense = nn.Linear(256, nr_char_tokens)\n",
    "        \n",
    "    # character to index for lookup in char embedding table\n",
    "    def charToIndex(self, char):\n",
    "        return self.unique_chars.index(char)\n",
    "\n",
    "    # first forward call: interm_outputs shoudl be a tensor with the embedding of <BOS>\n",
    "    def forward(self, input_img, target_seq):\n",
    "        # through 5 convolutional layers\n",
    "        # first conv\n",
    "        encoder_out = self.layerNorm1(self.maxPool(self.leakyRelu(self.conv1(input_img))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # second conv\n",
    "        encoder_out = self.layerNorm2(self.maxPool(self.leakyRelu(self.conv2(encoder_out))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # third conv\n",
    "        encoder_out = self.layerNorm3(self.maxPool(self.leakyRelu(self.conv3(encoder_out))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # forth conv\n",
    "        encoder_out = self.layerNorm4(self.leakyRelu(self.conv4(encoder_out)))\n",
    "        # fifth conv\n",
    "        encoder_out = self.layerNorm5(self.leakyRelu(self.conv5(encoder_out)))\n",
    "\n",
    "        # flatten layer\n",
    "        encoder_out = self.layerNorm6(self.leakyRelu(self.flattenConv(encoder_out)))\n",
    "\n",
    "        # dense layer (activation function not mentioned in paper) \n",
    "        # needs reshaped tensor where dims are reversed ((batch, 128, 1, x) -> (batch, x, 1, 128))\n",
    "        encoder_out = torch.reshape(encoder_out, (encoder_out.size(0), encoder_out.size(3), encoder_out.size(2), encoder_out.size(1)))\n",
    "        encoder_out = self.dense1(encoder_out)\n",
    "\n",
    "        # add sinusodial positional information\n",
    "        # needs reshape (batch, seq_len, 1, 256) -> (seq_len, batch, 256)  \n",
    "        encoder_out = torch.reshape(encoder_out, (encoder_out.size(1), encoder_out.size(0), encoder_out.size(3)))\n",
    "        encoder_out = self.encoder_pos_encoding(encoder_out)\n",
    "\n",
    "        # transformer encoder layers\n",
    "        encoder_out = self.trans_encoder1(encoder_out)\n",
    "        encoder_out = self.trans_encoder2(encoder_out)\n",
    "        encoder_out = self.trans_encoder3(encoder_out)\n",
    "        encoder_out = self.trans_encoder4(encoder_out)\n",
    "\n",
    "        # encoder output for CTC Loss\n",
    "        interm_encoder_out = encoder_out\n",
    "\n",
    "        # add sinusodial positional information again\n",
    "        encoder_out = self.encoder_pos_encoding(encoder_out)\n",
    "\n",
    "        # target sequence (shifted right (so with <BOS> token))\n",
    "        decoder_in = self.char_embedding(target_seq)\n",
    "        \n",
    "        # add sinusoidal positional information to decoder input\n",
    "        decoder_in = self.decoder_pos_encoding(decoder_in)\n",
    "        \n",
    "        print(\"decoder in:\", decoder_in.shape)\n",
    "        \n",
    "        # input encoder output and predicted chars into decoder (TODO: teacher forcing)\n",
    "        decoder_out = self.trans_decoder1(decoder_in, encoder_out)\n",
    "        decoder_out = self.trans_decoder2(decoder_out, encoder_out)\n",
    "        decoder_out = self.trans_decoder3(decoder_out, encoder_out)\n",
    "        decoder_out = self.trans_decoder4(decoder_out, encoder_out)\n",
    "\n",
    "        # dense layer after decoder to predict one of all tokens\n",
    "        print(\"decoder out:\", decoder_out.shape)\n",
    "        decoder_out = self.decoder_out_dense(decoder_out)\n",
    "        print(\"dense out:\", decoder_out.shape)\n",
    "\n",
    "        return interm_encoder_out, decoder_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize and pad images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizes to largest width in batch x 128, keeping aspect ratio and padding image\n",
    "def resizeBatch(images, image_width, image_height):\n",
    "    resized_batch = torch.empty((images.size(0), 1, image_height, image_width), dtype = torch.float32)\n",
    "    resize_transform = tv.transforms.Resize((image_height, image_width), antialias = True)\n",
    "\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        resized = resize_transform(image)\n",
    "        resized_batch[idx] = resized\n",
    "\n",
    "    return resized_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder in: torch.Size([56, 1, 256])\n",
      "decoder out: torch.Size([56, 1, 256])\n",
      "dense in: torch.Size([56, 1, 56])\n",
      "torch.Size([277, 1, 256]) torch.Size([56, 1, 56])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hwr_transformer = HWRTransformer(input_width, INPUT_HEIGHT, longest_label, char_to_idx_mapping)\n",
    "\n",
    "test_image, test_label = train_set.__getitem__(2)\n",
    "# print(\"Mean of image: %.2f\" % test_image.float().mean().item())\n",
    "# print(test_label)\n",
    "\n",
    "# create \"batches\" with single image / label \n",
    "test_image_batch = test_image.unsqueeze(0)\n",
    "\n",
    "# resize\n",
    "resized_batch = resizeBatch(test_image_batch, input_width, INPUT_HEIGHT)\n",
    "# print(resized_batch.size())\n",
    "\n",
    "plt.imshow(resized_batch[0, 0, :, :], cmap = \"gray\")\n",
    "\n",
    "# test label = <BOS> *sentence in tokens* <EOS> <PAD> <PAD> ... \n",
    "out1, out2 = hwr_transformer(resized_batch, test_label)\n",
    "print(out1.shape, out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
