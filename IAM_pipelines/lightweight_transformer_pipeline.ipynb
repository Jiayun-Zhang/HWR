{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Transformer-based Model for Handwritten Character Recognition \n",
    "(https://hal.science/hal-03685976/file/A_Light_Transformer_Based_Architecture_for_Handwritten_Text_Recognition.pdf)\n",
    "\n",
    "## ***note: Has a CNN backbone***\n",
    "\n",
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture  \n",
    "Build up with a double Transformer architecture:  \n",
    "- Image transformer as encoder: Extracts the visual features\n",
    "- Text transformer as decoder: Language modeling\n",
    "- Encoder: \n",
    "- Decoder: Generates word-sections sequence using visual features and previous predictions\n",
    "\n",
    "### Encoder:  \n",
    "- CNN Backbone (5 convolutions)\n",
    "- Sinusodial position encoding  \n",
    "- 4 layer transformer layer encoder\n",
    "\n",
    "### Decoder: \n",
    "- Takes encoder output and along with sequence of previously predicted characters\n",
    "- Additional loss in the middle of the network to help convergence\n",
    "\n",
    "--------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torchvision as tv\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"/home/hkolstee/uniprojects/DATA/HWR/IAM-data/IAM-data/\"\n",
    "TRAIN_TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "INPUT_HEIGHT = 128\n",
    "# input width -> largest width in batch\n",
    "# padded to get to width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_names</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a03-017-07.png</td>\n",
       "      <td>into the pro-communist north and the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a03-017-05.png</td>\n",
       "      <td>to 1958 kept the kingdom in peace, though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a03-017-08.png</td>\n",
       "      <td>pro-western centre and south.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a03-017-02.png</td>\n",
       "      <td>in Phnom Penh indicate that he still regards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a03-017-06.png</td>\n",
       "      <td>at the cost of virtual partition of the country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7453</th>\n",
       "      <td>d06-000-08.png</td>\n",
       "      <td>fears are based upon completely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7454</th>\n",
       "      <td>d06-000-05.png</td>\n",
       "      <td>is worrying them, to find the original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7455</th>\n",
       "      <td>d06-000-09.png</td>\n",
       "      <td>irrational pre-conceived notions - or to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7456</th>\n",
       "      <td>d06-000-02.png</td>\n",
       "      <td>already suggested, not to be silly or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7457</th>\n",
       "      <td>d06-000-00.png</td>\n",
       "      <td>In the first place it is not a great deal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7458 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           img_names                                           labels\n",
       "0     a03-017-07.png             into the pro-communist north and the\n",
       "1     a03-017-05.png        to 1958 kept the kingdom in peace, though\n",
       "2     a03-017-08.png                    pro-western centre and south.\n",
       "3     a03-017-02.png     in Phnom Penh indicate that he still regards\n",
       "4     a03-017-06.png  at the cost of virtual partition of the country\n",
       "...              ...                                              ...\n",
       "7453  d06-000-08.png                  fears are based upon completely\n",
       "7454  d06-000-05.png           is worrying them, to find the original\n",
       "7455  d06-000-09.png         irrational pre-conceived notions - or to\n",
       "7456  d06-000-02.png            already suggested, not to be silly or\n",
       "7457  d06-000-00.png        In the first place it is not a great deal\n",
       "\n",
       "[7458 rows x 2 columns]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_fwf(DATA_PATH + \"iam_lines_gt.txt\", header = None)\n",
    "raw_data = raw_data.values.tolist()\n",
    "\n",
    "data = {'img_names': np.squeeze(raw_data[::2]),\n",
    "        'labels': np.squeeze(raw_data[1::2])}\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Data augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = TRAIN_TEST_SPLIT)\n",
    "\n",
    "# reset indices from current random state\n",
    "train.reset_index(inplace = True)\n",
    "test.reset_index(inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom pytorch dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we need the input/image width we have to resize the images to.  \n",
    "This is the largest image width in the entire batch of images (source paper randomly added/removed new augments each training epoch).   \n",
    "For now we just take the largest width in the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBiggestWidth(data: pd.DataFrame):\n",
    "    biggest_width = 0\n",
    "\n",
    "    for index in range(len(data['img_names'])):\n",
    "        image_path = os.path.join(DATA_PATH, \"img\", data['img_names'][index])\n",
    "        image = read_image(image_path)\n",
    "        \n",
    "        if (image.size(2) > biggest_width):\n",
    "            biggest_width = image.size(2)\n",
    "\n",
    "    return biggest_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260\n"
     ]
    }
   ],
   "source": [
    "input_width = getBiggestWidth(data)\n",
    "print(input_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandWritingDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, img_width, img_height, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # function to get patches of tokens from original input image\n",
    "    # def __getTokens__(self, image):\n",
    "        # pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # input image\n",
    "        image_path = os.path.join(DATA_PATH, \"img\", self.data['img_names'][index])\n",
    "        # torchvision read_image call\n",
    "        image = read_image(image_path)\n",
    "        # resize to \n",
    "        \n",
    "        \n",
    "        # string label\n",
    "        label = self.data['labels'][index]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        # return length of column\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = HandWritingDataset(train, input_width, INPUT_HEIGHT, BATCH_SIZE)\n",
    "test_set = HandWritingDataset(test, input_width, INPUT_HEIGHT, BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinusodial positional encoding  \n",
    "(can be changed to nn.embedding layers if we don't get good results, however that is not exactly sinusodial pos encoding like in the paper I think)\n",
    "\n",
    "<!-- **CHANGED TO NN.EMBEDDING IN MODEL**   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinPosEncoding(nn.Module):\n",
    "    def __init__(self, dimensionality):\n",
    "        super(SinPosEncoding, self).__init__()\n",
    "        self.dims = dimensionality\n",
    "        self.max_len = 5000\n",
    "\n",
    "        # position vector\n",
    "        positions = torch.arange(0, self.max_len).unsqueeze(1)\n",
    "        # calculate added angle for sin/cos\n",
    "        angle = torch.exp(torch.arange(0, self.dims, 1) * (-np.log(10000.0) / self.dims))\n",
    "\n",
    "        # initialize the 2D positional encodings array\n",
    "        pos_encodings = torch.zeros(self.max_len, 1, self.dims)\n",
    "        # calucalte encodings\n",
    "        pos_encodings = torch.sin(positions * angle)\n",
    "\n",
    "        # add to buffer for training performance (?)\n",
    "        self.register_buffer('pos_encodings', pos_encodings)\n",
    "\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # adds the positional encoding elementwise to the tensor (seqlength, batch, embeddims)\n",
    "        input += self.pos_encodings[0:input.size(1)]\n",
    "\n",
    "        return input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For character level embedding (decoder input) we find out how many characters are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNrOfCharacters(data: pd.DataFrame):\n",
    "    return set(data['labels'].apply(list).sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of unique characters in the labels of the dataset: 79\n",
      "{')', 'D', 'i', 'g', '5', '?', 'r', '/', 'A', 'k', '-', 'a', 'W', 'N', 'm', 'E', 'C', 'n', 'M', 'H', 'U', 'V', '2', 'e', 'f', 's', '0', '!', 'p', '*', 'P', ';', ':', 'R', 'K', 'd', ',', '&', '7', 'J', ' ', '.', 'I', 'c', '#', 'Z', 'Q', '\"', 'j', 'G', 'x', 'F', '(', 'z', 'w', 'l', '6', '1', 't', 'b', 'h', 'X', 'B', '9', 'L', 'v', 'Y', 'T', '3', 'u', '4', '+', 'o', 'S', 'y', 'q', \"'\", '8', 'O'}\n"
     ]
    }
   ],
   "source": [
    "unique_chars = findNrOfCharacters(data)\n",
    "print(\"Nr of unique characters in the labels of the dataset:\", len(unique_chars))\n",
    "print(unique_chars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWRTransformer(nn.Module):\n",
    "    def __init__(self, input_width, input_height, embedding_dim, batch_size):\n",
    "        super(HWRTransformer, self).__init__()\n",
    "\n",
    "        # convolutional block (5 convolutions)\n",
    "        # first convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3))\n",
    "        width = input_width - 2\n",
    "        height = input_height - 2\n",
    "        self.leakyRelu = nn.LeakyReLU()     # reuse in later layers\n",
    "        self.maxPool = nn.MaxPool2d((2,2))  # reuse in later layers\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm1 = nn.LayerNorm(normalized_shape = [8, height, width])\n",
    "        self.dropout = nn.Dropout(0.2)      # reuse in later layers\n",
    "\n",
    "        # second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # after maxpool\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm2 = nn.LayerNorm(normalized_shape = [16, height, width])\n",
    "\n",
    "\n",
    "        # third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # after maxpool\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm3 = nn.LayerNorm(normalized_shape = [32, height, width])\n",
    "\n",
    "        # forth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # no maxpool\n",
    "        self.layerNorm4 = nn.LayerNorm(normalized_shape = [64, height, width])\n",
    "\n",
    "        # fifth convolutional layer (kernel size to better match shape of character)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (4, 2))\n",
    "        width -= 1\n",
    "        height -= 3\n",
    "        # no maxpool\n",
    "        self.layerNorm5 = nn.LayerNorm(normalized_shape = [128, height, width])\n",
    "\n",
    "        # following is convolution with width 1 which is used to flatten the current output\n",
    "        self.flattenConv = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (height, 1))\n",
    "        self.layerNorm6 = nn.LayerNorm(normalized_shape = [128, 1, width])\n",
    "\n",
    "        # dense layer to upscale from 128 to 256\n",
    "        self.dense1 = nn.Linear(in_features = 128, out_features = 256)\n",
    "        # sinusoidal positional encoding is added to the output of the dense layer\n",
    "        self.pos_encoding = SinPosEncoding(dimensionality = 256)\n",
    "\n",
    "        # transformer encoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
    "        self.trans_encoder1 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder2 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder3 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder4 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "\n",
    "        # Here starts: decoder\n",
    "        # character embedding \n",
    "        self.char_embedding = nn.Embedding()\n",
    "\n",
    "        # transformer decoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
    "        self.trans_decoder1 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder2 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder3 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder4 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "\n",
    "    def forward(self, input_img, interm_outputs):\n",
    "        # through 5 convolutional layers\n",
    "        # first conv\n",
    "        encoder_out = self.layerNorm1(self.maxPool(self.leakyRelu(self.conv1(input_img))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # second conv\n",
    "        encoder_out = self.layerNorm2(self.maxPool(self.leakyRelu(self.conv2(encoder_out))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # third conv\n",
    "        encoder_out = self.layerNorm3(self.maxPool(self.leakyRelu(self.conv3(encoder_out))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # forth conv\n",
    "        encoder_out = self.layerNorm4(self.leakyRelu(self.conv4(encoder_out)))\n",
    "        # fifth conv\n",
    "        encoder_out = self.layerNorm5(self.leakyRelu(self.conv5(encoder_out)))\n",
    "\n",
    "        # flatten layer\n",
    "        encoder_out = self.layerNorm6(self.leakyRelu(self.flattenConv(encoder_out)))\n",
    "\n",
    "        # dense layer (activation function not mentioned in paper) \n",
    "        # needs reshaped tensor where dims are reversed ((1, 128, 1, x) -> (1, x, 1, 128))\n",
    "        encoder_out = torch.reshape(encoder_out, (encoder_out.size(0), encoder_out.size(3), encoder_out.size(2), encoder_out.size(1)))\n",
    "        encoder_out = self.dense1(encoder_out)\n",
    "\n",
    "        # add sinusodial positional information\n",
    "        # needs reshape (batch, seq_len, 1, 256) -> (seq_len, batch, 256)  \n",
    "        encoder_out = torch.reshape(encoder_out, (encoder_out.size(1), encoder_out.size(0), encoder_out.size(3)))\n",
    "        encoder_out = self.pos_encoding(encoder_out)\n",
    "\n",
    "        # transformer encoder layers\n",
    "        encoder_out = self.trans_encoder1(encoder_out)\n",
    "        encoder_out = self.trans_encoder2(encoder_out)\n",
    "        encoder_out = self.trans_encoder3(encoder_out)\n",
    "        encoder_out = self.trans_encoder4(encoder_out)\n",
    "\n",
    "        # encoder output for CTC Loss\n",
    "        output1 = encoder_out\n",
    "\n",
    "        # add sinusodial positional information again\n",
    "        encoder_out = self.pos_encoding(encoder_out)\n",
    "\n",
    "        # input encoder output and predicted chars into decoder (TODO: teacher forcing)\n",
    "        decoder_out = self.trans_decoder1(encoder_out, interm_outputs)\n",
    "        decoder_out = self.trans_decoder2(encoder_out, interm_outputs)\n",
    "        decoder_out = self.trans_decoder3(encoder_out, interm_outputs)\n",
    "        decoder_out = self.trans_decoder4(encoder_out, interm_outputs)\n",
    "\n",
    "        return output1, decoder_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize and pad images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizes to largest width in batch x 128, keeping aspect ratio and padding image\n",
    "def resizeBatch(images, image_width, image_height):\n",
    "    resized_batch = torch.empty((images.size(0), 1, image_height, image_width), dtype = torch.float32)\n",
    "    resize_transform = tv.transforms.Resize((image_height, image_width), antialias = True)\n",
    "\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        resized = resize_transform(image)\n",
    "        resized_batch[idx] = resized\n",
    "\n",
    "    return resized_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[430], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# print(resized_batch.size())\u001b[39;00m\n\u001b[1;32m     14\u001b[0m plt\u001b[39m.\u001b[39mimshow(resized_batch[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :, :], cmap \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m out1, out2 \u001b[39m=\u001b[39m hwr_transformer(resized_batch)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(out1\u001b[39m.\u001b[39msize(), out2\u001b[39m.\u001b[39msize())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[428], line 114\u001b[0m, in \u001b[0;36mHWRTransformer.forward\u001b[0;34m(self, input_img)\u001b[0m\n\u001b[1;32m    111\u001b[0m encoder_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoding(encoder_out)\n\u001b[1;32m    113\u001b[0m \u001b[39m# input encoder output and target transcription (when training) / predicted chars (after training) into decoder\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m decoder_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrans_decoder1(encoder_out)\n\u001b[1;32m    115\u001b[0m decoder_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrans_decoder2(encoder_out)\n\u001b[1;32m    116\u001b[0m decoder_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrans_decoder3(encoder_out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'memory'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAABNCAYAAACMq59FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAud0lEQVR4nO2deVRUR77Hv91NL0DTNDTQzQ6tCIiKgoLEJRpRMMYtalSMMXkx26iJSZyX+LKYl5M8JzqZl0wmMZPkzagvEzVuiRqXuOKGgCDKJrigoOwNTdPQ0N331vsj59ajBaMmrFqfc/oo91bfrrpVt+pbv/rV74oIIQQMBoPBYDAYvQhxT2eAwWAwGAwG41aYQGEwGAwGg9HrYAKFwWAwGAxGr4MJFAaDwWAwGL0OJlAYDAaDwWD0OphAYTAYDAaD0etgAoXBYDAYDEavgwkUBoPBYDAYvQ4mUBgMBoPBYPQ6mEBhMBgMBoPR6+hRgfL5558jJCQECoUC8fHxyMjI6MnsMBgMBoPB6CX0mEDZsmULXnvtNaxatQrZ2dmIjo5GUlISqqureypLDAaDwWAwegminnpZYHx8PEaMGIG//e1vAACe5xEYGIhly5bhzTff7IksMRgMBoPB6CU49cSPWq1WZGVlYeXKlfSYWCxGYmIi0tLS2qVvbW1Fa2sr/ZvnedTV1UGj0UAkEnVLnhkMBoPBYPw+CCFobGyEn58fxOJfX8TpEYFSW1sLjuOg1Wodjmu1Wly8eLFd+tWrV+M///M/uyt7DAaDwWAwupCysjIEBAT8apoeESj3ysqVK/Haa6/RvxsaGhAUFISysjKoVKoezBnjt1JdXY0bN24gICAAPj4+7c4TQph1jMFgMLoQQggELw9CCDIyMtDQ0IDExESIxWKIRKJO7Ye/+OIL5OTkYMuWLXBzc7tj+h4RKF5eXpBIJKiqqnI4XlVVBZ1O1y69XC6HXC5vd1ylUjGB0svpSGjU19fj22+/xahRo+Dj49OuDu12OzZv3owRI0ZgwIABTKgwHGhtbYVMJmPtgsH4nbR1QbVarWhsbERraytcXV0hk8nA8zwIIZBIJJ3yvLm7u9PfvJvr9cguHplMhtjYWBw+fJge43kehw8fRkJCQk9kidFF3NoICSHIz89HXV0d4uPjO1TREokEPj4+2Lp1KyorK7srq4w+QGtrK3Jzc2Gz2Xo6KwxGn0ewkAiWlMrKSohEIlitVvA8DwB39BO5Fzw8PNDc3HzX6Xtsiee1117DokWLMHz4cMTFxeGTTz5BU1MTnnnmmZ7KEqOLaGtFsVqtyM3NRVBQEBQKRYcqWiQSYfz48ejXrx+uXbsGLy8vSKXS7s42o5dBCEFBQQF27NiBQYMG9XR2GIw+DSEEdrsdEokEhBCIxWIolUoEBgZCKpXSvrkzl3mUSiWcnO5edvSYQJk7dy5qamrw7rvvorKyEkOHDsX+/fvbOc4y+j5tG3dLSwsKCgoQHx/voMxvXQqSSqXo168f9Ho9M+UzAPzSjkwmE86dOweLxQKFQtHTWWIw+iwikYiKE7vdDp7nYTKZIJPJHNJ1pj+gwWDoGwIFAJYuXYqlS5f+5u8LZilCCKqrq6FUKqFUKjsxh4zOhBCCmzdv4vr160hJSXEQKLd7AJg4YbRFp9NBJpOxJR4G43diNptx9epVyOVyyGQy2O12FBcXIz4+ngoWsVgMiUQCoHOWeoxG4z1Zw/vELp7bwXEcOI5DU1MTtm3bhkmTJmHAgAE9nS3GbRAEipOTEwIDA5n4YNwzZrPZodO8n+F5HkajESqV6p5mnQ8SHMfBbrdDKpV2qq9EX+G3WjesVisyMjJw9uxZmM1m2O12mEwmZGVlISgoCBaLBV5eXggICIBCoYBYLKbGALPZDJFIBKVSec+/bbPZoFar7zp9n271PM+D53nk5OQgNzcXc+fO7eks3RVtt3Z19jau3gzHcbhy5Qr8/PygUqkemHLfLTzPP1Dt4V7heR61tbWQyWQd7urri7S2tqK+vr7D3YtXr17FX/7yF7z88ssIDw/vknZhs9l6hX8XIQRNTU2or6+HRqOBi4vLHdMXFhYiOzsbCoUC0dHRCAsLo31rXxMrra2tMJlM8PLyovVss9lQX18PqVQKtVpNj/M8D7vdjvz8fGRmZiIyMhLDhw+HTCajZRccXwUriEgkgt1up/8CQEBAAGQyGV1Gr6yspAHUWltbkZ+fj2vXrkGpVCI0NBQeHh7IyclBVlYW5HI5xo4di8jISPpcGo1GBAQE0LrraBLR3Nz84FhQ7HY7zGYz9u/fj6ioKLi6urZLY7FYsG/fPgQGBiI2NpYOADzPw2q1orW1FXK5vEvWs9uqW47j0NjYiOzsbFy9ehUtLS3geR6urq4YNGgQBg8efMeHsi/C8zyam5tx8+ZNlJaW4vDhw9Dr9bhw4QJ8fX1pZ3TruueDgs1mQ0VFBQoKClBaWorIyEjEx8c7OKn1FL1NMHEchwsXLiAoKKidQBFmdmazuU85VRsMBhw4cADz58936INaWlqwb98+SKVSNDc3g+M4akWxWq04evQoVCpVO1+ulpYWnDx5EiKRCCqVCpGRkXBxcelwwLZardixYwcmTZoEDw8P2i+2trbCbDajtbUVUqkUnp6eXXo/jUYjUlNTkZWVhYyMDCQkJGDevHkICQm5rRCtqqrC3/72Nzz55JOIiIiAs7MzOI6jg3FnYrPZ0NzcjKamJkgkEnh7e3e6ACotLcX27dvx5JNPwsfHB0ajEadOnYLFYkFZWRmmTp2KiIgIiMViWK1W5OXlISMjA66urti2bRuioqJon1FeXo6cnByEhIQgODiYbkYQorGnp6cjMDAQwcHBcHZ2hqurKyQSCaqrq6FWqzF69Ggapd1ms6GlpQUGgwHp6ekoKyvD+PHj4ePjA47jUFFRgZycHDQ1NUEqlaKyshJeXl4IDg7uMAQIIaTv+KD8XiQSCUpLS9HQ0ICFCxe2G+Q4joPZbMauXbswa9YsGj/BZrOhvLwcqampyMnJgUqlwgsvvHDHqHb3StsOvqqqCunp6fDw8MAjjzwCV1dXmo89e/bgzJkzeO655yCXyx22fjU1NcFoNMLZ2fk3hfY3m81wdnZ2UNVVVVXIzc1FaGgo+vXrB+CXhtPS0oK6ujpIJBJoNJrbiobbDVwcx0EkEjk8vOXl5di+fTtSU1PR0tKCS5cuQa1W4/vvv4fdbodcLseIESMwceJEeHt73+Md7tuYzWYcPHgQDQ0N0Ov1iIiIQG1tLfbu3YtHH320S0Wb1WpFfn4+xGIxBg8eTOtMeI1EVlYWCgoKIJfLsXDhwg63g5vNZvA8Dzc3t3tul4QQNDQ0oKamBjabDb6+vg6zxI4wGAw4d+4cHn/8cTo7E56RrKws7N69G5cuXcKjjz6KlJSUDvMszLB7k/AqKChARUUFQkND6bG6ujrcuHEDI0aMQH19PYD/j1lx5coVVFdXIzo62mFW3djYiCNHjqChoQEPP/wwlEolrl27hvLycjzyyCPtBgYhMNegQYPg7u5OBeDRo0fR3NwMlUoFqVQKrVaLWbNmdfqgTAhBbW0tvvnmG6jVajz11FNISUlBbW0t/vWvfyEyMhKzZs2Ck5NTu98Wyp2VlQWbzYbg4GB4eHjAxcUFIpHody+JcRyH5uZmVFRU4OLFi5BKpQgMDIRcLoeHhweMRiOUSiWcnZ07pR25urpCq9XCbDYDAPbu3YvQ0FCMHDkSp0+fxo4dO/DGG29ALBajuroaZ86cwahRo1BYWIiEhATIZDJwHIeWlhbk5eUhMDAQOp0OUqkUTk5O4DgOMpkMJSUlKCgowNChQyGRSODl5QWO4wD8EjxTLpfD1dWVlsnJyQktLS34+uuvUVNTg1deeQWhoaGQSqW4efMmfvjhBwQHB+Ohhx6Ck5MTiouL8e233+IPf/hDO4FCCHF4Zc3d0KcFis1mQ15eHvz8/Doc3HieR2lpKdRqNcLDw8FxHGpqanD27FkYDAaEh4cjJiYGmzdvxmeffYYPPvigU2cKEokEdrsdNpsN27dvR3h4OKKjo2nF2e12eHt7IyAgAB988AFOnjwJFxcXhIWFwdXVFXl5eTh79izc3NygVCqRmJgId3d3AL9UtsViQV1dHXUY9Pb2dnASNplM2LRpE6ZOnQqdTkffgbB3716IxWIEBATQRp2bm4uSkhKHmdOcOXOg0Wjo9TiOQ3FxMc6cOYPk5GRotVrY7XYYjUacPn0aGRkZ8Pb2xksvvQSFQgGO41BYWIhBgwYhOTkZBw8eRG5uLt555x0olUrwPI+GhgYUFxejvLwcra2tMBqNnbqFVBjArl27BoPBgODgYAQFBfWoCVhw6t6wYQNUKhUmTpwIqVSK7Oxs5OXlQaFQoKWlpcsECsdxSE9Px969ezF//nw6aJtMJmRnZyMzMxOenp6Ijo7GJ598glGjRiE6Orrddfbs2QMfHx88/PDD7YSpUM7Kykpcv34dw4YNo7Ph+vp6pKamIjc3ly7XqNVqPPLII7f1TeJ5Hvn5+WhqaqIDs9C+tmzZAr1ej1mzZiErKwunT59GUlISFSiCSbygoAAZGRkYMGAAhg0bdttlRp7nUV9fD2dnZ2rVFMzlQMem69+KWCyGVCqly9XCWn9RURE0Gg3UajUqKiqoed5ms+Hs2bMIDAyEp6cnzb/ZbMbmzZvh6+uLxx57DC4uLmhtbcXx48chkUhgs9naDdpisRg8z+PatWsICAjAwYMHcenSJYwfPx79+/eng961a9c6rbxtqa2txfbt2xEREYHx48dTcRESEoKMjAwUFxejtbW1w/vt4+OD999/H4WFhbh27RrOnz+PhoYGyGQyTJs2DeHh4b/5Ged5HlVVVTh+/Dj0ej1GjhwJFxcXyOVy2Gw2FBQUUJ/HkSNHdoq10263o66uDjKZDOnp6dDr9RgyZAhcXFyg1+tx9uxZ2O12ODk5IS0tDWq1Gt7e3rh8+TIVG4Lg27lzJ1asWAG5XA6e56mvpmCpGj58uEPbF4vFsNvtqKiogFarpYJGIpGA4zj8+OOPyM7OxvLly6HX6+Hk5ITy8nJs2LABw4cPpwKprKwM+/fvR0lJCX1W2iL0EU1NTXd9X/q0QGloaEB6ejpmzJhBzVQChBAYjUZs3rwZw4YNg0ajQWFhIbZt24bhw4djwoQJcHNzg0QiweTJk/Hxxx/DbDbDw8OjU/ModAB5eXkYOXIk5HI5fXCkUikIIVAoFCCEoLy8HDdu3IBGo0FGRgYKCwsxbdo0BAUFQSKR0IYjiCyTyUQVd1ZWFubMmYPExET6205OTnSweeyxxwAAly5dgkwmw8SJE6FWq1FXV4dNmzahsbER8+bNg6enJ4qKirB7924YDAYHgWIwGPDll19i8ODBcHNzg9lsxoULF3Ds2DGo1WqEhYVh06ZNmDt3Lvz8/CCRSDB+/HhwHIe6ujoUFBTgoYcegqenJyQSCd137+fnh+rqanz44YeYNm0aFShmsxnl5eUICgqCTCajHarBYMD58+fR0tKChIQEeHh4tOuMBKev8+fP0zXTYcOGQalUUlNwTyAIgc2bN0MkEmHSpEkAgJ9++gnOzs6IjIzEqVOnYLfbuyzcv9lsxu7duxEeHg69Xk+F/E8//QQXFxfMnj0bXl5eKC8vp0uRHZWjpKSEds7C/RTyLPiGZWdnIyYmhrb1srIy/Pd//zfCwsLw7LPPQqPRQCKRwGKxwGg00vLeWnabzYa0tDRERUVBp9NBJBKhpqYGH3/8MWJiYiCRSPD111/D1dUVzz77rINPB8dxOHToEDZt2oQ5c+Zg2LBhEIvFOHv2LMLCwto57d28eROffvopXnzxRfTr148OSocPH4ZMJkNKSorDc/F7kEgk9COIoObmZpw/fx4DBw6EQqGAyWSiIrK8vBw2m42a9IFf2vqhQ4dgMBgwffp0KBQKGAwG/PzzzygsLERgYCD1O2iLWCyGi4sLysrKcPToUWRmZuKJJ57AgAED4OLiQv0ZYmJiOqWsbbHZbNi1axc8PT0xfvx4OmkTLLzXr19Hv379bisGRSIRNBoNHnroIcTFxYHjODQ0NODAgQP4+OOP8cEHH3To13MneJ7HuXPnkJ+fj5iYGISEhNCJZlFREa5duwaFQgGFQoHCwkKMHDnyniKj3g6LxUItgTU1NYiLi6POqcLSnzDZPXfuHGbMmAGlUomkpCTqIGyz2SAWi9HY2IiysjLodDrwPA+LxYIbN27g1KlT8PX1RWRkJP1dod3ZbDZUVVVBq9VSa7tEIkFdXR2OHj2K2NhYxMbGQi6X03hE9fX1GDx4MOrr65GXl4erV68iICAAZ86cua0IcXJyenB8UM6dOweO4xAZGUmXL9qaPDMzM8FxHAYMGIAdO3bg8uXLGDduHOLi4uDi4gInJyfYbDa6dbGjh/j3wHEceJ7H5cuXYbPZ4OPj4xCiW4jYl5+fj+bmZnh5eeHw4cM4deoUysrKMH/+fOrIRAgBx3HIysqiyl6pVCI9PR08z2PGjBkYMWKEw+8rFAoMHToUWVlZSEpKAgBkZmZSKw7Hcfjuu+9gNpuxcOFCNDY2YsOGDSguLsbMmTPh5+dHr8XzPDIyMlBXV4dHHnkENTU12LVrF+RyOWbNmgVfX1/k5OTAZDJRk6FQRuAX56iWlhZERkZSsSWY2VtbW5GdnY3r169j8ODBAP7fCW7jxo1444034O3tjZqaGqSlpeHmzZtwcXFBZmYmMjMz8frrr7dbZigtLcU//vEPaLVazJw5E/7+/rDb7Th16hSqq6sxefJkeHp6drtQEWYkNpsNycnJuHLlCkpLS9G/f38MGTKE3tOu3LVhNBphNBoxfPhwVFRU4NixYygqKsK4ceMwZswY6rMgrE935BslEong5uaG6upqh3DZwgCbmpqK48ePY/bs2RgyZAjEYjEsFgs2btwIf39/LFiwAG5ubvT+u7m5OSzJ3NrZm0wmXLx4EQsXLqQd9o0bN1BVVQWNRoMbN25g8eLFGDhwIJydnR2sT7m5udi8eTMWLlyIhIQESKVSHDp0CDt27MCyZcswdOhQh34jIyMDIpEIPj4+qKqqwq5du3Dx4kWMGzcObm5u92ym/jUkEgmsVqvDMsbNmzfR0NCAgQMHorq6GkajERaLBTU1NThy5IiDz4hwb44cOYIFCxbAZrMhPT0dVVVVCA8Ph1qtxokTJ351W/bFixdx8OBBzJ07l4oTYTLUVc9HfX09zp8/j1deeQVyuRx2ux319fWorKxEVlYWtXoJVqWOEAZRiUQCnufh5OSE2NhY7Nu3D0aj8TcJlPLycmzbtg1PP/00tFotXea5cOECZDIZIiMjERgYiLKyMmRmZtJ4PL/XiiL46V29ehWvvPIK1Go1DS8vlUphs9lgtVqhUCggl8vR1NREx7vW1lbqUFtcXAxXV1d8//33DjFNjEYjoqKiMGDAADoGtfWPJIRAKpXCx8eHTjiEMScyMhLTp093cDHw9PRERUUF1q5dC19fX0REROCxxx5DU1MTtm7diqqqqg4t4YI1727p0wLlxIkTiI+Ph1KppI1UaMzNzc34+eef4eHhgd27dyMoKAiLFy+GVqulfh7CjJzjOPpQdjaCaXbQoEFwcXGhKldYbjl58iQyMjKwePFiNDY2Uq/phQsXUo9ou90Ou92O2tpa/M///A9SUlKQnp4OiUSCGTNmICoqCh4eHu3yLxKJMGDAAJw+fRpNTU0wm830fUfCbCw9PR3Tp0/H5s2bUVhYiOjoaLz66qt0e5lAU1MT9u7di3HjxsFgMOCnn36CWq3GtGnT4OHhAY7jUF5e3u69SYJfSkVFBfU1EOpKMCHabDbk5OTQtXDhvglOYjzPIz09HampqejXrx9mzpwJd3d3KBQKfPXVV0hJSXHY8kYIwa5duyCRSDB//ny4ubnBYDBgz549KC8vR0NDAyQSCWbPnt3tzrl1dXXYv38/JkyYgB07dkCj0SAxMRH+/v4ghODq1avw9PSks+nO3k7LcRz27t0LjUaD4uJipKamIiwsDC+88AJ8fX3p/RCJRGhpaYFIJIKzs3OH1+rfvz++//57TJ06FRqNBo2Njbhw4QL27t0Li8WCp556CkOGDKFluHHjBrKysvDhhx9CqVTe0+B3/vx5hISEID4+nl5PGNj8/f2RnJxMhUnbe8ZxHLZt24bBgwdj5MiRVOQfOXIEZrMZpaWlDnlsbW3FiRMnEBERgerqanz77bdQKBRYsmQJAgICOt15WfCJq62thU6ng91uR1lZGaKioqhfiNlsRllZGbZu3QqFQgFfX1+HZ11Ypk1LS4OTkxP0ej0mTpwIpVKJmzdv4qeffsKBAwcwa9asDtv76dOnodFoMHToUHq+M6wCdyq33W5HRkYGdDodamtraV0kJydj1KhRNN/h4eEYOHBgu/YiDKyC39Tx48dx8uRJTJ069Tf7E7bdeMHzPEJCQhAREUHvp9BO9Ho9tm7dioqKCuj1+t99P8xmM65fv45XX30Ver2eWkV4nodOp8OyZcvodvOEhARs374d58+fh1QqhcVigUwmg1arxeDBg/H222/j9OnTuHbtGnx8fDBgwADExsZCoVBAJpPRawu+OsIy45QpU+Ds7OzwvPv7++M//uM/4Ozs7GD5iIuLw/vvvw+TyYTAwED6PDc1NSEwMBAFBQWYMGFCu3JqtVqcO3furu9LnxYoFosFY8eOpQ9V2wZss9lgMpkgFosxZ84c9O/fH87OztREJaSVyWRobGyEq6trl2xdlEgkKCsrg5+fH3VWbW5uRl5eHn7++WeoVCo8/fTT0Gg02LRpE8xmMxISEjBkyBDaUJycnOjSQFVVFTw9PfH000/D3d29XYd8K76+vmhsbERJSQmys7OhUqng4eEBnudRUVGBvLw86HQ6DBs2DLNnz4a3tzdtxG2pqanBpUuXoFAocOzYMcycORNjx46lDlWC81ZISIjDTNjJyQlWqxXV1dWIiYmBSqVyCLBHCKEmwieffJLWQUtLC06fPg0/Pz+sW7cOdrsdKSkp6N+/P6RSKex2O4YPH45//vOfuHTpEvr3709/kxACg8EAu92OkpISXLx4EdevX0d8fDxmzpyJzZs345tvvsHYsWM73TH6btDpdCgvL8ekSZMQGRkJZ2dnSCQSNDU1obW1Fd7e3l0W58NiseDEiROwWCxQqVRYvHgx+vfvT9erxWIx7bwUCgXc3d1vK9zj4+Nx+PBh/PnPf4ZKpUJxcTGkUimmTZuGUaNGwdvb2+G7Tk5OUKvVcHFxcXC0FmaCHMfBZDLBYDBApVLRqNKCH9GTTz7psHbu7+8PX19fbNmyBb6+vtQ8LSxJCctKJpMJQUFBuHLlCvbv3w+DwYAnnngC69atQ21trYMQFCYCCoUCGzZsgN1ux7PPPkt3BnW2RUEmk0GpVOL69evo378/iouL8eOPP+KFF16As7Mz/P39ERcXh2+//RYqlQqLFi1q92x6eXnhww8/hMlkglarpe1JLBZDr9djzZo1aG5u7nACo1QqQQjB66+/TkOcd2ZgrtuhVquxcuVKulQdFhbmsOzu4+ODf/u3f0NpaeltnbBra2tRWFiIwsJCVFZW0vYcFBR0W1F9J4KDg/Huu+/CbDZDqVTSCZNw74R2GxkZiYCAAGzYsAHLly//3dZYkUiEESNGYNiwYZDJZPT3xGIxjRslXH/ixImIiYlBfX09rFYr7dMVCgWtu9mzZzv4TLXN260BMoW/w8LCOsxbR6JWKpUiIiLCwW9KcFdYsWJFuxcBC8yZMwcBAQHYuXPnXd2XPi1QXnzxRboeDTiqfXd3d7z33nuQyWTUG72tMBFm9oIFRXAO6kyEQEsGgwF6vR65ubmwWq0oKSlBc3MzkpOT0a9fP7qj58qVKxgzZgySkpIgl8uphUFYMvHz84OHhwe2bNmCl19++Y4hiUUiETw8PKBUKnHlyhUcPHgQ8+fPp57d7u7u0Gg0mDlzJoYOHUo7J6vVCqvViubmZmi1WrplMSYmBhqNBs888wxCQ0PpoCaUVSQStXNWFolEkMlkGDVqFEaPHk07m7b78zmOQ3R0NOLi4mj9CGvMN2/eRHJyMpKSkuDt7U0fKMHBOCIiAunp6Rg3bhx1EBaLxZg5cyb+93//Fzt27MCwYcOwaNEiar6MjY3Fxo0bUVJSAn9//27dzeHl5YX333+fBhsTBhthsLbb7bRT6gqR4uLighUrVoDjOERERFCBKQzSbU2/arUaI0aMuO398fDwwFtvvYWrV6+iubkZc+bMga+vLxU1t37P29sbgYGBWLNmDSZNmgS1Wo3m5ma65b+hoQGNjY0YNmyYg3leJBLh0UcfbXdNtVqNt99+G//4xz/wX//1X/T56NevHwYOHIiIiAhIJBIkJydj9+7dKCkpwahRozB8+HAoFAqo1Wo0NjY6LFG1tLTAaDQiOzsbJSUlWLNmDby9vWn/0dlIpVKMHDmSWge/+eYbDBkyBHq9nu4QmTZtGhITEzucOAj3R6vVdviaELFYDC8vrw5/WyQSIS4uDm5ubnSGfeuzCXRuqPO2BAYGIjAwsMNzYrEYarX6tkG9BAdsJycnTJgwAV5eXnQCKCz7/BaEPqxtPyaUX3irL8dxUKvVeOutt3Do0CF89913mDJlyu8KEhofH4+YmBjqd/JrCALOx8fnV9N0Jb92j4cOHXrb73l4eGD06NF3/Tsi0hVPXRdjMpng7u4Oo9FIFW5HCCZAYVYIOG4zFNLU1taitrYWUVFRnfYgCluqjh8/jrS0NEyYMAEtLS1Qq9UICAiAUqmkzrFCPi5dugSNRgMfHx+H11sLeeZ5Hnl5efj0008hkUiQkpICf39/KmJ0Ol07gWA0GvHhhx/i4sWL0Ol0ePPNN2lHZrVasXbtWly5cgVz586Fu7s76uvracCgyMhIhIWF0YdTWJ4SBrG290pYsuJ5/o7bRTvi1rVmwcmZEEJ9FYR8CPcL+GXZIDs7G4mJiQ6dmd1uh9VqBfBLZydYznieR3FxMZ566im8/PLLWLhwYbdvN701mJTwt9Vqxbp161BZWYmUlBR4eHggICCgR3cc/ZoPwL0iWDOysrJQVVUFq9UKkUgEX19f+hFe8363v8nzPLWWChEu3dzc6KQE+MWaarFYqIOesA6+a9cuHDhwAKtWraID5aVLl/DEE09Ao9Fg+fLlGDduHKRSqUMQrM7GaDRi48aNKC4uhkajweLFi6mTeXfQVQLkQUBof101obgfEcbvhoaGDmOltOWeBcrx48exdu1aZGVloaKiAjt37sSMGTPoeUIIVq1aha+//hpGoxGjRo3CunXrHMxHdXV1WLZsGXbv3g2xWIxZs2bh008/vev36NxLAXsKoTP+7LPPMGTIEDz88MMOSzZtG/XdvtZaED319fU4ffo0SktLQQiBp6cnoqKiEBUV1c6h0Wq1ori4GDt27MD06dPbbb8zm804evQodTxVq9WIiopCYGAgFApFnw2xfWu0XsFCIOwSeOqppzBmzBi89dZbPbqj59aBwWAw4MSJExCJRBg8eDBCQ0PZ4NEJdHSvLRYL8vPz6VIR8EuMkWXLliElJQXTp093cFjuynYibLEXnJL7WiRUBuNu6VKBsm/fPpw6dQqxsbF4/PHH2wmUjz76CKtXr8aGDRsQGhqKd955B7m5uSgoKKBOl5MnT0ZFRQX+/ve/w2az4ZlnnsGIESPw3XffdXoBewpCCIqLi/HRRx/hzTffRHBwsIMVQLDqtN3x8lvEwN3Ofh7EWVLbXQjCfRcChC1fvhxWqxXr16+/b8KmM34/whtd7/W18AwG4+64l/H7np/AyZMnY/LkyR2eI4Tgk08+wdtvv43p06cDADZu3AitVosffvgB8+bNQ2FhIfbv34/MzEwMHz4cAPDZZ5/h0UcfxZ///GeHra19GUIILl68iMjISLqeLpgA2zoHCmLl96yZdma6+4lb76lwz6VSKcLCwpCZmdlhjA/Gg4vg+8BgMHqeTrUjlpSUoLKy0iFYmLu7O+Lj45GWlgYANAqeIE4AIDExEWKxGOnp6R1eV3iRUttPb6elpQWFhYV4+OGH4erq2s4ZUvi/sM2rD7oC9Una3ue7cUhjMBgMRs/Qqb1zZWUlALTzJtdqtfRcZWVlO+9jJycneHp60jS3snr1ari7u9PP7Ty/exNGoxFWqxUhISH0WFu/CIG2AaIYXYNwz9vefyFCKhOGDAaD0TvpE9PHlStXoqGhgX7Kysp6Okt3xG6346GHHqIxR9rufOlIpDAP8K7j1u2SVqsVN27cgFarZfedwWAweimd6gUm+FpUVVVRr3jhb2FvtE6nQ3V1tcP3hBcl3S408a3RSfsCQUFBCAoKcjj2IPqB9AZu3c1TWlqKoqIiPP/880ygMBgMRi+lUy0ooaGh0Ol0OHz4MD1mMpmQnp6OhIQEAEBCQgKMRiOysrJomiNHjoDnecTHx3dmdhgMh91Lwts+//WvfyEkJATjxo1jopHBYDB6KfdsQTGbzbh8+TL9u6SkBDk5OfD09ERQUBCWL1+ODz74AGFhYXSbsZ+fH92KHBkZieTkZDz33HP48ssvYbPZsHTpUsybN+++2cHD6D0IAeaamppQVFSEH3/8EZWVlVixYsVvepkYg8FgMLqHe46DcuzYMYwfP77d8UWLFmH9+vU0UNtXX30Fo9GI0aNH44svvnAIA1xXV4elS5c6BGr761//eteB2hoaGqBWq1FWVtZr46Aweh6bzYbi4mLk5eXBaDSirq4Oer0eCQkJ8PHxgZOTE4t1wWAwGN2I8ILBO0WCB/poqPurV6+iX79+PZ0NBoPBYDAYv4GysrI7vqy1T04fPT09AQClpaV3VGCM3oOgnJnlq2/B6q3vweqsb/Ig1Jvw3ra7cenokwJFCK7l7u5+31bi/YxKpWL11gdh9db3YHXWN7nf6+1uDQt9Ig4Kg8FgMBiMBwsmUBgMBoPBYPQ6+qRAkcvlWLVqVZ8L3vagw+qtb8Lqre/B6qxvwurNkT65i4fBYDAYDMb9TZ+0oDAYDAaDwbi/YQKFwWAwGAxGr4MJFAaDwWAwGL0OJlAYDAaDwWD0OvqkQPn8888REhIChUKB+Ph4ZGRk9HSWHljee+89iEQih09ERAQ939LSgiVLlkCj0UCpVGLWrFmoqqpyuEZpaSmmTJkCFxcX+Pj44I9//CPsdnt3F+W+5vjx45g6dSr8/PwgEonwww8/OJwnhODdd9+Fr68vnJ2dkZiYiEuXLjmkqaurw4IFC6BSqaBWq/Hss8/CbDY7pLlw4QLGjBkDhUKBwMBArFmzpquLdt9ypzp7+umn2z17ycnJDmlYnXU/q1evxogRI+Dm5gYfHx/MmDEDRUVFDmk6q188duwYYmJiIJfL0b9/f6xfv76ri9et9DmBsmXLFrz22mtYtWoVsrOzER0djaSkJFRXV/d01h5YoqKiUFFRQT8nT56k51599VXs3r0bW7duRWpqKsrLy/H444/T8xzHYcqUKbBarTh9+jQ2bNiA9evX49133+2Joty3NDU1ITo6Gp9//nmH59esWYO//vWv+PLLL5Geng5XV1ckJSWhpaWFplmwYAHy8/Nx8OBB7NmzB8ePH8fzzz9Pz5tMJkyaNAnBwcHIysrC2rVr8d577+Grr77q8vLdj9ypzgAgOTnZ4dnbtGmTw3lWZ91PamoqlixZgjNnzuDgwYOw2WyYNGkSmpqaaJrO6BdLSkowZcoUjB8/Hjk5OVi+fDkWL16MAwcOdGt5uxTSx4iLiyNLliyhf3McR/z8/Mjq1at7MFcPLqtWrSLR0dEdnjMajUQqlZKtW7fSY4WFhQQASUtLI4QQsnfvXiIWi0llZSVNs27dOqJSqUhra2uX5v1BBQDZuXMn/ZvneaLT6cjatWvpMaPRSORyOdm0aRMhhJCCggICgGRmZtI0+/btIyKRiNy8eZMQQsgXX3xBPDw8HOrtjTfeIOHh4V1covufW+uMEEIWLVpEpk+fftvvsDrrHVRXVxMAJDU1lRDSef3iv//7v5OoqCiH35o7dy5JSkrq6iJ1G33KgmK1WpGVlYXExER6TCwWIzExEWlpaT2YswebS5cuwc/PD3q9HgsWLEBpaSkAICsrCzabzaG+IiIiEBQUROsrLS0NgwcPhlarpWmSkpJgMpmQn5/fvQV5QCkpKUFlZaVDPbm7uyM+Pt6hntRqNYYPH07TJCYmQiwWIz09naYZO3YsZDIZTZOUlISioiLU19d3U2keLI4dOwYfHx+Eh4fjpZdegsFgoOdYnfUOGhoaAPz/S247q19MS0tzuIaQ5n4aC/uUQKmtrQXHcQ6VBgBarRaVlZU9lKsHm/j4eKxfvx779+/HunXrUFJSgjFjxqCxsRGVlZWQyWRQq9UO32lbX5WVlR3Wp3CO0fUI9/nXnqvKykr4+Pg4nHdycoKnpyeryx4iOTkZGzduxOHDh/HRRx8hNTUVkydPBsdxAFid9QZ4nsfy5csxatQoDBo0CAA6rV+8XRqTyQSLxdIVxel2+uTbjBm9h8mTJ9P/DxkyBPHx8QgODsb3338PZ2fnHswZg3F/M2/ePPr/wYMHY8iQIejXrx+OHTuGCRMm9GDOGAJLlixBXl6eg18e4+7pUxYULy8vSCSSdt7OVVVV0Ol0PZQrRlvUajUGDBiAy5cvQ6fTwWq1wmg0OqRpW186na7D+hTOMboe4T7/2nOl0+naOaLb7XbU1dWxuuwl6PV6eHl54fLlywBYnfU0S5cuxZ49e3D06FEEBATQ453VL94ujUqlum8mh31KoMhkMsTGxuLw4cP0GM/zOHz4MBISEnowZwwBs9mMK1euwNfXF7GxsZBKpQ71VVRUhNLSUlpfCQkJyM3NdehIDx48CJVKhYEDB3Z7/h9EQkNDodPpHOrJZDIhPT3doZ6MRiOysrJomiNHjoDnecTHx9M0x48fh81mo2kOHjyI8PBweHh4dFNpHlxu3LgBg8EAX19fAKzOegpCCJYuXYqdO3fiyJEjCA0NdTjfWf1iQkKCwzWENPfVWNjTXrr3yubNm4lcLifr168nBQUF5PnnnydqtdrB25nRfbz++uvk2LFjpKSkhJw6dYokJiYSLy8vUl1dTQgh5MUXXyRBQUHkyJEj5OzZsyQhIYEkJCTQ79vtdjJo0CAyadIkkpOTQ/bv30+8vb3JypUre6pI9yWNjY3k3Llz5Ny5cwQA+ctf/kLOnTtHrl+/Tggh5E9/+hNRq9Xkxx9/JBcuXCDTp08noaGhxGKx0GskJyeTYcOGkfT0dHLy5EkSFhZG5s+fT88bjUai1WrJwoULSV5eHtm8eTNxcXEhf//737u9vPcDv1ZnjY2NZMWKFSQtLY2UlJSQQ4cOkZiYGBIWFkZaWlroNViddT8vvfQScXd3J8eOHSMVFRX009zcTNN0Rr949epV4uLiQv74xz+SwsJC8vnnnxOJREL279/freXtSvqcQCGEkM8++4wEBQURmUxG4uLiyJkzZ3o6Sw8sc+fOJb6+vkQmkxF/f38yd+5ccvnyZXreYrGQP/zhD8TDw4O4uLiQmTNnkoqKCodrXLt2jUyePJk4OzsTLy8v8vrrrxObzdbdRbmvOXr0KAHQ7rNo0SJCyC9bjd955x2i1WqJXC4nEyZMIEVFRQ7XMBgMZP78+USpVBKVSkWeeeYZ0tjY6JDm/PnzZPTo0UQulxN/f3/ypz/9qbuKeN/xa3XW3NxMJk2aRLy9vYlUKiXBwcHkueeeazdRY3XW/XRUZwDIP//5T5qms/rFo0ePkqFDhxKZTEb0er3Db9wPiAghpLutNgwGg8FgMBi/Rp/yQWEwGAwGg/FgwAQKg8FgMBiMXgcTKAwGg8FgMHodTKAwGAwGg8HodTCBwmAwGAwGo9fBBAqDwWAwGIxeBxMoDAaDwWAweh1MoDAYDAaDweh1MIHCYDAYDAaj18EECoPBYDAYjF4HEygMBoPBYDB6HUygMBgMBoPB6HX8H3DCJOKao2HSAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hwr_transformer = HWRTransformer(input_width, INPUT_HEIGHT)\n",
    "\n",
    "test_image, _ = train_set.__getitem__(1)\n",
    "# print(\"Mean of image: %.2f\" % test_image.float().mean().item())\n",
    "\n",
    "# create \"batch\" with single image resize\n",
    "test_image_batch = test_image.unsqueeze(0)\n",
    "# print(test_image_batch.size())\n",
    "\n",
    "# resize\n",
    "resized_batch = resizeBatch(test_image_batch, input_width, INPUT_HEIGHT)\n",
    "# print(resized_batch.size())\n",
    "\n",
    "plt.imshow(resized_batch[0, 0, :, :], cmap = \"gray\")\n",
    "\n",
    "out1, out2 = hwr_transformer(resized_batch)\n",
    "print(out1.size(), out2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
