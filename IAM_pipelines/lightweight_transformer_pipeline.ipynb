{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Transformer-based Model for Handwritten Character Recognition \n",
    "(https://hal.science/hal-03685976/file/A_Light_Transformer_Based_Architecture_for_Handwritten_Text_Recognition.pdf)\n",
    "\n",
    "## ***note: Has a CNN backbone***\n",
    "\n",
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture  \n",
    "Build up with a double Transformer architecture:  \n",
    "- Image transformer as encoder: Extracts the visual features\n",
    "- Text transformer as decoder: Language modeling\n",
    "- Encoder: \n",
    "- Decoder: Generates word-sections sequence using visual features and previous predictions\n",
    "\n",
    "### Encoder:  \n",
    "- CNN Backbone (5 convolutions)\n",
    "- Sinusodial position encoding  \n",
    "- 4 layer transformer layer encoder\n",
    "\n",
    "### Decoder: \n",
    "- Takes encoder output and along with sequence of previously predicted characters\n",
    "- Additional loss in the middle of the network to help convergence\n",
    "\n",
    "--------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_image\n",
    "import torchvision as tv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"/home/hkolstee/uniprojects/DATA/HWR/IAM-data/IAM-data/\"\n",
    "TRAIN_TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "INPUT_HEIGHT = 128\n",
    "# input width -> largest width in batch\n",
    "# padded to get to width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_names</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a03-017-07.png</td>\n",
       "      <td>into the pro-communist north and the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a03-017-05.png</td>\n",
       "      <td>to 1958 kept the kingdom in peace, though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a03-017-08.png</td>\n",
       "      <td>pro-western centre and south.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a03-017-02.png</td>\n",
       "      <td>in Phnom Penh indicate that he still regards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a03-017-06.png</td>\n",
       "      <td>at the cost of virtual partition of the country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7453</th>\n",
       "      <td>d06-000-08.png</td>\n",
       "      <td>fears are based upon completely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7454</th>\n",
       "      <td>d06-000-05.png</td>\n",
       "      <td>is worrying them, to find the original</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7455</th>\n",
       "      <td>d06-000-09.png</td>\n",
       "      <td>irrational pre-conceived notions - or to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7456</th>\n",
       "      <td>d06-000-02.png</td>\n",
       "      <td>already suggested, not to be silly or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7457</th>\n",
       "      <td>d06-000-00.png</td>\n",
       "      <td>In the first place it is not a great deal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7458 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           img_names                                           labels\n",
       "0     a03-017-07.png             into the pro-communist north and the\n",
       "1     a03-017-05.png        to 1958 kept the kingdom in peace, though\n",
       "2     a03-017-08.png                    pro-western centre and south.\n",
       "3     a03-017-02.png     in Phnom Penh indicate that he still regards\n",
       "4     a03-017-06.png  at the cost of virtual partition of the country\n",
       "...              ...                                              ...\n",
       "7453  d06-000-08.png                  fears are based upon completely\n",
       "7454  d06-000-05.png           is worrying them, to find the original\n",
       "7455  d06-000-09.png         irrational pre-conceived notions - or to\n",
       "7456  d06-000-02.png            already suggested, not to be silly or\n",
       "7457  d06-000-00.png        In the first place it is not a great deal\n",
       "\n",
       "[7458 rows x 2 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_fwf(DATA_PATH + \"iam_lines_gt.txt\", header = None)\n",
    "raw_data = raw_data.values.tolist()\n",
    "\n",
    "data = {'img_names': np.squeeze(raw_data[::2]),\n",
    "        'labels': np.squeeze(raw_data[1::2])}\n",
    "        # 'labels': [\"<BOS>\" + str(list(string)) + \"<EOS>\" for string in np.squeeze(raw_data[1::2])]}\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Data augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size = TRAIN_TEST_SPLIT)\n",
    "\n",
    "# reset indices from current random state\n",
    "train.reset_index(inplace = True)\n",
    "test.reset_index(inplace = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom pytorch dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, we need the input/image width we have to resize the images to.  \n",
    "This is the largest image width in the entire batch of images (source paper randomly added/removed new augments each training epoch).   \n",
    "For now we just take the largest width in the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBiggestWidth(data: pd.DataFrame):\n",
    "    biggest_width = 0\n",
    "\n",
    "    for index in range(len(data['img_names'])):\n",
    "        image_path = os.path.join(DATA_PATH, \"img\", data['img_names'][index])\n",
    "        image = read_image(image_path)\n",
    "        \n",
    "        if (image.size(2) > biggest_width):\n",
    "            biggest_width = image.size(2)\n",
    "\n",
    "    return biggest_width\n",
    "\n",
    "def getLongestLabel(data: pd.DataFrame):\n",
    "    longest = 0\n",
    "    \n",
    "    for index in range(len(data['labels'])):\n",
    "        if (len(data['labels'][index]) > longest):\n",
    "            longest = len(data['labels'][index])\n",
    "            \n",
    "    return longest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biggest width needed to pad all images to this width for the input into the encoder.  \n",
    "Longest label needed to pad all labels to this length for the input into the decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "input_width = getBiggestWidth(data)\n",
    "print(input_width)\n",
    "\n",
    "longest_label = getLongestLabel(data)\n",
    "# <BOS> and <EOS> tokens not counted\n",
    "longest_label += 2\n",
    "print(longest_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For character level embedding (decoder input) we find out how many characters are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dict of uniques chars sorted on how common they are in the dataset labels\n",
    "def uniqueCharsByMostCommon(data: pd.DataFrame):\n",
    "    sortedDict = OrderedDict(Counter(''.join(data['labels'].values[4:-4])).most_common())\n",
    "    newDict = {}\n",
    "    \n",
    "    # first add pad, begin of sentence, and end of sentence tokens\n",
    "    newDict[\"<PAD>\"] = 0\n",
    "    newDict[\"<BOS>\"] = 1\n",
    "    newDict[\"<EOS>\"] = 2\n",
    "    \n",
    "    for idx, char in enumerate(sortedDict):\n",
    "        newDict[char] = idx + 3\n",
    "    \n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandWritingDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, img_width, img_height, char_to_idx_mapping, max_label_size, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.char_to_idx_mapping = char_to_idx_mapping\n",
    "\n",
    "        # change character level strings to embed indices \n",
    "        #   (embedding itself calculated in forward pass)\n",
    "        self.labels_as_idxs = [torch.tensor([[self.char_to_idx_mapping[char]] for char in label]) for label in data['labels']]   \n",
    "        # add <BOS> and <EOS> tokens at begin and end of sentences\n",
    "        self.labels_as_idxs = [torch.cat([torch.tensor([[self.char_to_idx_mapping['<BOS>']]]), label]) for label in self.labels_as_idxs]\n",
    "        self.labels_as_idxs = [torch.cat([label, torch.tensor([[self.char_to_idx_mapping['<EOS>']]])]) for label in self.labels_as_idxs]\n",
    "        \n",
    "        # pad labels to largest label length with <pad> token\n",
    "        # print(self.labels_as_idxs[0].shape)\n",
    "        self.labels_as_idxs = [F.pad(label, (0, 0, 0, max_label_size - label.shape[0]), mode = 'constant', value = self.char_to_idx_mapping['<PAD>']) for label in self.labels_as_idxs]\n",
    "        # print(self.labels_as_idxs[0].shape)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # input image\n",
    "        image_path = os.path.join(DATA_PATH, \"img\", self.data['img_names'][index])\n",
    "        # torchvision read_image call\n",
    "        image = read_image(image_path)\n",
    "        # resize to \n",
    "        \n",
    "        # string label\n",
    "        label = self.labels_as_idxs[index]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        # return length of column\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<BOS>': 1,\n",
       " '<EOS>': 2,\n",
       " ' ': 3,\n",
       " 'e': 4,\n",
       " 't': 5,\n",
       " 'a': 6,\n",
       " 'o': 7,\n",
       " 'n': 8,\n",
       " 'i': 9,\n",
       " 's': 10,\n",
       " 'r': 11,\n",
       " 'h': 12,\n",
       " 'l': 13,\n",
       " 'd': 14,\n",
       " 'c': 15,\n",
       " 'u': 16,\n",
       " 'm': 17,\n",
       " 'f': 18,\n",
       " 'w': 19,\n",
       " 'p': 20,\n",
       " 'g': 21,\n",
       " 'y': 22,\n",
       " 'b': 23,\n",
       " '.': 24,\n",
       " ',': 25,\n",
       " 'v': 26,\n",
       " 'k': 27,\n",
       " \"'\": 28,\n",
       " '\"': 29,\n",
       " '-': 30,\n",
       " 'T': 31,\n",
       " 'I': 32,\n",
       " 'M': 33,\n",
       " 'A': 34,\n",
       " 'S': 35,\n",
       " 'B': 36,\n",
       " 'P': 37,\n",
       " 'H': 38,\n",
       " 'W': 39,\n",
       " 'C': 40,\n",
       " 'N': 41,\n",
       " 'G': 42,\n",
       " 'x': 43,\n",
       " 'R': 44,\n",
       " 'L': 45,\n",
       " 'E': 46,\n",
       " 'D': 47,\n",
       " 'F': 48,\n",
       " '0': 49,\n",
       " '1': 50,\n",
       " 'j': 51,\n",
       " 'O': 52,\n",
       " 'q': 53,\n",
       " '!': 54,\n",
       " 'U': 55,\n",
       " '(': 56,\n",
       " 'K': 57,\n",
       " '?': 58,\n",
       " 'z': 59,\n",
       " '3': 60,\n",
       " ')': 61,\n",
       " ';': 62,\n",
       " '9': 63,\n",
       " 'V': 64,\n",
       " '2': 65,\n",
       " 'J': 66,\n",
       " 'Y': 67,\n",
       " ':': 68,\n",
       " '5': 69,\n",
       " '8': 70,\n",
       " '4': 71,\n",
       " '6': 72,\n",
       " '#': 73,\n",
       " '&': 74,\n",
       " '7': 75,\n",
       " '/': 76,\n",
       " 'Q': 77,\n",
       " 'X': 78,\n",
       " '*': 79,\n",
       " 'Z': 80,\n",
       " '+': 81}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get mapping before splitting dataset\n",
    "char_to_idx_mapping = uniqueCharsByMostCommon(data)\n",
    "\n",
    "train_set = HandWritingDataset(train, input_width, INPUT_HEIGHT, char_to_idx_mapping, longest_label, BATCH_SIZE)\n",
    "test_set = HandWritingDataset(test, input_width, INPUT_HEIGHT, char_to_idx_mapping, longest_label, BATCH_SIZE)\n",
    "\n",
    "char_to_idx_mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinusodial positional encoding  \n",
    "(can be changed to nn.embedding layers if we don't get good results, however that is not exactly sinusodial pos encoding like in the paper I think)\n",
    "\n",
    "<!-- **CHANGED TO NN.EMBEDDING IN MODEL**   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinPosEncoding(nn.Module):\n",
    "    def __init__(self, dimensionality):\n",
    "        super(SinPosEncoding, self).__init__()\n",
    "        self.dims = dimensionality\n",
    "        self.max_len = 5000\n",
    "\n",
    "        # position vector\n",
    "        positions = torch.arange(0, self.max_len).unsqueeze(1)\n",
    "        # calculate added angle for sin/cos\n",
    "        angle = torch.exp(torch.arange(0, self.dims, 1) * (-np.log(10000.0) / self.dims))\n",
    "\n",
    "        # initialize the 2D positional encodings array\n",
    "        pos_encodings = torch.zeros(self.max_len, 1, self.dims)\n",
    "        # calucalte encodings\n",
    "        pos_encodings = torch.sin(positions * angle)\n",
    "\n",
    "        # add to buffer for training performance (?)\n",
    "        self.register_buffer('pos_encodings', pos_encodings)\n",
    "\n",
    "    \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        print(input.shape)\n",
    "        print(self.pos_encodings.shape)\n",
    "        # adds the positional encoding elementwise to the tensor (seqlength, batch, embeddims)\n",
    "        input += self.pos_encodings[0:input.size(1)]\n",
    "        print(\"succes\")\n",
    "\n",
    "        return input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWRTransformer(nn.Module):\n",
    "    def __init__(self, input_width, input_height, max_label_length, char_to_idx_mapping):\n",
    "        super(HWRTransformer, self).__init__()\n",
    "        # create unique char to idx mapping\n",
    "        self.char_to_idx_mapping = char_to_idx_mapping\n",
    "        self.idx_to_char_mapping = {value: key for key, value in char_to_idx_mapping.items()}\n",
    "        # print(self.idx_to_char_mapping)\n",
    "\n",
    "        # convolutional block (5 convolutions)\n",
    "        # first convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (3,3))\n",
    "        width = input_width - 2\n",
    "        height = input_height - 2\n",
    "        self.leakyRelu = nn.LeakyReLU()     # reuse in later layers\n",
    "        self.maxPool = nn.MaxPool2d((2,2))  # reuse in later layers\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm1 = nn.LayerNorm(normalized_shape = [8, height, width])\n",
    "        self.dropout = nn.Dropout(0.2)      # reuse in later layers\n",
    "\n",
    "        # second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # after maxpool\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm2 = nn.LayerNorm(normalized_shape = [16, height, width])\n",
    "\n",
    "\n",
    "        # third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # after maxpool\n",
    "        width = int(np.floor(width/2))\n",
    "        height = int(np.floor(height/2))\n",
    "        self.layerNorm3 = nn.LayerNorm(normalized_shape = [32, height, width])\n",
    "\n",
    "        # forth convolutional layer\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (3, 3))\n",
    "        width -= 2\n",
    "        height -= 2\n",
    "        # no maxpool\n",
    "        self.layerNorm4 = nn.LayerNorm(normalized_shape = [64, height, width])\n",
    "\n",
    "        # fifth convolutional layer (kernel size to better match shape of character)\n",
    "        self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = (4, 2))\n",
    "        width -= 1\n",
    "        height -= 3\n",
    "        # no maxpool\n",
    "        self.layerNorm5 = nn.LayerNorm(normalized_shape = [128, height, width])\n",
    "\n",
    "        # following is convolution with width 1 which is used to flatten the current output\n",
    "        self.flattenConv = nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = (height, 1))\n",
    "        self.layerNorm6 = nn.LayerNorm(normalized_shape = [128, 1, width])\n",
    "\n",
    "        # dense layer to upscale from 128 to 256\n",
    "        self.dense1 = nn.Linear(in_features = 128, out_features = 256)\n",
    "        # sinusoidal positional encoding is added to the output of the dense layer\n",
    "        self.encoder_pos_encoding = SinPosEncoding(dimensionality = 256)\n",
    "\n",
    "        # transformer encoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
    "        self.trans_encoder1 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder2 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder3 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_encoder4 = nn.TransformerEncoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "\n",
    "        # Here starts: decoder\n",
    "        # character embedding (dim rule of thumb -> 4th sqrt of nr_embeddings: for ~80 = 3)\n",
    "        # <PAD> embedding idx = 0\n",
    "        self.char_embedding = nn.Embedding(len(self.char_to_idx_mapping), 3, padding_idx = 0)\n",
    "        # positional embedding of decoder input sequence (-1 because shifted right)\n",
    "        self.decoder_pos_encoding = SinPosEncoding(dimensionality = max_label_length - 1)\n",
    "\n",
    "        # transformer decoder layers (4 stacked transformer encoder layers (4 headed attention))\n",
    "        self.trans_decoder1 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder2 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder3 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        self.trans_decoder4 = nn.TransformerDecoderLayer(d_model = 256, nhead = 4, dim_feedforward = 1024, dropout = 0.2)\n",
    "        \n",
    "    # character to index for lookup in char embedding table\n",
    "    def charToIndex(self, char):\n",
    "        return self.unique_chars.index(char)\n",
    "\n",
    "    # first forward call: interm_outputs shoudl be a tensor with the embedding of <BOS>\n",
    "    def forward(self, input_img, target_seq):\n",
    "        # through 5 convolutional layers\n",
    "        # first conv\n",
    "        encoder_out = self.layerNorm1(self.maxPool(self.leakyRelu(self.conv1(input_img))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # second conv\n",
    "        encoder_out = self.layerNorm2(self.maxPool(self.leakyRelu(self.conv2(encoder_out))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # third conv\n",
    "        encoder_out = self.layerNorm3(self.maxPool(self.leakyRelu(self.conv3(encoder_out))))\n",
    "        encoder_out = self.dropout(encoder_out)\n",
    "        # forth conv\n",
    "        encoder_out = self.layerNorm4(self.leakyRelu(self.conv4(encoder_out)))\n",
    "        # fifth conv\n",
    "        encoder_out = self.layerNorm5(self.leakyRelu(self.conv5(encoder_out)))\n",
    "\n",
    "        # flatten layer\n",
    "        encoder_out = self.layerNorm6(self.leakyRelu(self.flattenConv(encoder_out)))\n",
    "\n",
    "        # dense layer (activation function not mentioned in paper) \n",
    "        # needs reshaped tensor where dims are reversed ((batch, 128, 1, x) -> (batch, x, 1, 128))\n",
    "        encoder_out = torch.reshape(encoder_out, (encoder_out.size(0), encoder_out.size(3), encoder_out.size(2), encoder_out.size(1)))\n",
    "        encoder_out = self.dense1(encoder_out)\n",
    "\n",
    "        # add sinusodial positional information\n",
    "        # needs reshape (batch, seq_len, 1, 256) -> (seq_len, batch, 256)  \n",
    "        encoder_out = torch.reshape(encoder_out, (encoder_out.size(1), encoder_out.size(0), encoder_out.size(3)))\n",
    "        encoder_out = self.encoder_pos_encoding(encoder_out)\n",
    "\n",
    "        # transformer encoder layers\n",
    "        encoder_out = self.trans_encoder1(encoder_out)\n",
    "        encoder_out = self.trans_encoder2(encoder_out)\n",
    "        encoder_out = self.trans_encoder3(encoder_out)\n",
    "        encoder_out = self.trans_encoder4(encoder_out)\n",
    "\n",
    "        # encoder output for CTC Loss\n",
    "        output1 = encoder_out\n",
    "\n",
    "        # add sinusodial positional information again\n",
    "        encoder_out = self.encoder_pos_encoding(encoder_out)\n",
    "\n",
    "        # target sequence shifted right\n",
    "        decoder_in = self.char_embedding(target_seq)\n",
    "        # print(decoder_in.shape)\n",
    "        # print(encoder_out.shape)\n",
    "        \n",
    "        # add sinusoidal positional information to decoder input\n",
    "        #   (resize from (batch, seq_len, 1, embed_dim) -> (batch, embed_dim, 1, seq_len))\n",
    "        # decoder_in = torch.reshape(decoder_in, (decoder_in.size(2), decoder_in.size(1), decoder_in.size(0)))\n",
    "        # decoder_in = torch.reshape(decoder_in, (decoder_in.size(0), decoder_in.size(3), decoder_in.size(2), decoder_in.size(1)))\n",
    "        decoder_in = self.decoder_pos_encoding(decoder_in)\n",
    "        \n",
    "        # input encoder output and predicted chars into decoder (TODO: teacher forcing)\n",
    "        decoder_out = self.trans_decoder1(encoder_out, decoder_in)\n",
    "        decoder_out = self.trans_decoder2(encoder_out, decoder_in)\n",
    "        decoder_out = self.trans_decoder3(encoder_out, decoder_in)\n",
    "        decoder_out = self.trans_decoder4(encoder_out, decoder_in)\n",
    "\n",
    "        return output1, decoder_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize and pad images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizes to largest width in batch x 128, keeping aspect ratio and padding image\n",
    "def resizeBatch(images, image_width, image_height):\n",
    "    resized_batch = torch.empty((images.size(0), 1, image_height, image_width), dtype = torch.float32)\n",
    "    resize_transform = tv.transforms.Resize((image_height, image_width), antialias = True)\n",
    "\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        resized = resize_transform(image)\n",
    "        resized_batch[idx] = resized\n",
    "\n",
    "    return resized_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test with one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([277, 1, 256])\n",
      "torch.Size([5000, 256])\n",
      "succes\n",
      "torch.Size([277, 1, 256])\n",
      "torch.Size([5000, 256])\n",
      "succes\n",
      "torch.Size([55, 1, 3])\n",
      "torch.Size([5000, 55])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (55) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[261], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# print(resized_batch.size())\u001b[39;00m\n\u001b[1;32m     19\u001b[0m plt\u001b[39m.\u001b[39mimshow(resized_batch[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :, :], cmap \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m out1, out2 \u001b[39m=\u001b[39m hwr_transformer(resized_batch, test_target)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(out1\u001b[39m.\u001b[39msize(), out2\u001b[39m.\u001b[39msize())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[259], line 137\u001b[0m, in \u001b[0;36mHWRTransformer.forward\u001b[0;34m(self, input_img, target_seq)\u001b[0m\n\u001b[1;32m    129\u001b[0m decoder_in \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchar_embedding(target_seq)\n\u001b[1;32m    130\u001b[0m \u001b[39m# print(decoder_in.shape)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m# print(encoder_out.shape)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m# decoder_in = torch.reshape(decoder_in, (decoder_in.size(2), decoder_in.size(1), decoder_in.size(0)))\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# decoder_in = torch.reshape(decoder_in, (decoder_in.size(0), decoder_in.size(3), decoder_in.size(2), decoder_in.size(1)))\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m decoder_in \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder_pos_encoding(decoder_in)\n\u001b[1;32m    139\u001b[0m \u001b[39m# input encoder output and predicted chars into decoder (TODO: teacher forcing)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m decoder_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrans_decoder1(encoder_out, decoder_in)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[258], line 25\u001b[0m, in \u001b[0;36mSinPosEncoding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encodings\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     24\u001b[0m \u001b[39m# adds the positional encoding elementwise to the tensor (seqlength, batch, embeddims)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[39minput\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encodings[\u001b[39m0\u001b[39m:\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)]\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msucces\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (55) at non-singleton dimension 2"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hwr_transformer = HWRTransformer(input_width, INPUT_HEIGHT, longest_label, char_to_idx_mapping)\n",
    "\n",
    "test_image, test_label = train_set.__getitem__(2)\n",
    "# print(\"Mean of image: %.2f\" % test_image.float().mean().item())\n",
    "# print(test_label)\n",
    "\n",
    "# create \"batch\" with single image resize\n",
    "test_image_batch = test_image.unsqueeze(0)\n",
    "\n",
    "# shifted right target sequence as input for decoder\n",
    "test_target = test_label[:-1]\n",
    "# test_target_batch = test_target.unsqueeze(0)\n",
    "# print(test_target_idx)\n",
    "\n",
    "# resize\n",
    "resized_batch = resizeBatch(test_image_batch, input_width, INPUT_HEIGHT)\n",
    "# print(resized_batch.size())\n",
    "\n",
    "plt.imshow(resized_batch[0, 0, :, :], cmap = \"gray\")\n",
    "\n",
    "out1, out2 = hwr_transformer(resized_batch, test_target)\n",
    "print(out1.size(), out2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
